{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for train dataset\n",
    "\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "\n",
    "# Setup for test Dataset\n",
    "ds_test = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
    "ds_test = ds_train.batch(128)\n",
    "ds_test = ds_train.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input((28,28,1)),\n",
    "        layers.Conv2D(32,3,activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "optimizer = keras.optimizers.Adam()\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "acc_metric = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of Training Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python_39\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4929: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over epoch 0.9286500215530396\n",
      "\n",
      "Start of Training Epoch 1\n",
      "Accuracy over epoch 0.9755833148956299\n",
      "\n",
      "Start of Training Epoch 2\n",
      "Accuracy over epoch 0.9824000000953674\n",
      "\n",
      "Start of Training Epoch 3\n",
      "Accuracy over epoch 0.9862833619117737\n",
      "\n",
      "Start of Training Epoch 4\n",
      "Accuracy over epoch 0.9890833497047424\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nStart of Training Epoch {epoch}\")\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(ds_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch, training=True)\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "        acc_metric.update_state(y_batch, y_pred)\n",
    "    train_acc = acc_metric.result()\n",
    "    print(f\"Accuracy over epoch {train_acc}\")\n",
    "    acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over TEst Set: 0.9921500086784363\n"
     ]
    }
   ],
   "source": [
    "# Test Loop\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(ds_test):\n",
    "    y_pred = model(x_batch, training=False)\n",
    "    acc_metric.update_state(y_batch, y_pred)\n",
    "\n",
    "train_acc = acc_metric.result()\n",
    "print(f\"Accuracy over Test Set: {train_acc}\")\n",
    "acc_metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58a31560c3222f73c24c0eab0b6d9933bfafc2a96159f6aca9b232102d0e6606"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
