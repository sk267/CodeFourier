{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import  InputLayer, Conv2D, Lambda, Dropout, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from UNet_Fourier_Facilities import Fourier_Images\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654718587\n"
     ]
    }
   ],
   "source": [
    "print(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = tf.keras.models.load_model(\"./cnn_models/single_rgb_image_regression_V02_epochs_100_1653595623\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cnn = tf.keras.models.load_model(\"./models/single_rgb_image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img1 = Image.open(\n",
    "#     \"D:\\Main\\MA_PROGR\\Data\\Train\\LED_Wand_Aufnahmen\\Alias\\LED_Wand_20001.png\")\n",
    "# img1 = np.asarray(img1)/255\n",
    "# img1 = img1.reshape(1, 60, 60, 3)\n",
    "# res1 = model.predict(img1)\n",
    "# print(res1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILMED_PATH = \"D:\\\\Main\\\\MA_PROGR\\\\Data\\\\Train\\\\UNet_Train\\\\model_at_tree_100_pics\\\\filmed\"\n",
    "TRAIN_CLEAN_PATH = \"D:\\\\Main\\\\MA_PROGR\\\\Data\\\\Train\\\\UNet_Train\\\\model_at_tree_100_pics\\\\clean_aligned\"\n",
    "# TEST_PATH = \"./Data/data-science-bowl-2018/stage1_test/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filmed_imgs = []\n",
    "train_clean_imgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSET = 200\n",
    "abbruch_idx = 30\n",
    "\n",
    "\n",
    "def my_train_filmed_gen():\n",
    "    for i, addr_filmed in enumerate(os.listdir(TRAIN_FILMED_PATH)):\n",
    "        img =  plt.imread(f\"{TRAIN_FILMED_PATH}\\{addr_filmed}\")\n",
    "        yield img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3]\n",
    "\n",
    "def my_train_clean_gen():\n",
    "    for i, addr_clean in enumerate(os.listdir(TRAIN_CLEAN_PATH)):\n",
    "        img =  plt.imread(f\"{TRAIN_CLEAN_PATH}\\{addr_clean}\")\n",
    "        yield img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3]\n",
    "\n",
    "\n",
    "train_filmed_img_gen_obj = my_train_filmed_gen()\n",
    "train_clean_img_gen_obj = my_train_clean_gen()\n",
    "\n",
    "# for i, addr_filmed in enumerate( os.listdir(TRAIN_FILMED_PATH)):\n",
    "#     if i > 5:\n",
    "#         break\n",
    "#     img = plt.imread(f\"{TRAIN_FILMED_PATH}\\{addr_filmed}\")\n",
    "\n",
    "#     # :3 falls es Alpha Channel gibt (der soll weg)\n",
    "#     train_filmed_imgs.append(\n",
    "#         img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3])\n",
    "\n",
    "# for i , addr_clean in enumerate(os.listdir(TRAIN_CLEAN_PATH)):\n",
    "#     if i > 5:\n",
    "#         break\n",
    "#     # :3 falls es Alpha Channel gibt (der soll weg)\n",
    "#     img = plt.imread(f\"{TRAIN_CLEAN_PATH}\\{addr_clean}\")\n",
    "#     train_clean_imgs.append(\n",
    "#         img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_filmed_img_gen_obj.__next__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_clean_img_gen_obj.__next__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_clean_imgs[0])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build U-Net-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 128, 128, 1)  0           input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 128, 128, 16) 160         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 128, 128, 16) 0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 128, 128, 16) 2320        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 64, 64, 16)   0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 64, 64, 32)   4640        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 64, 64, 32)   0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 64, 32)   9248        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 32, 32, 32)   0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 32, 32, 64)   18496       max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32, 32, 64)   0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 32, 64)   36928       dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 16, 16, 64)   0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 16, 16, 128)  73856       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16, 16, 128)  0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 16, 16, 128)  147584      dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 8, 8, 128)    0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 256)    295168      max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 8, 8, 256)    0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 256)    590080      dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 16, 16, 128)  131200      conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 256)  0           conv2d_transpose_8[0][0]         \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 16, 16, 128)  295040      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 16, 16, 128)  0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 16, 16, 128)  147584      dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTrans (None, 32, 32, 64)   32832       conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 128)  0           conv2d_transpose_9[0][0]         \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32, 32, 64)   0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 32, 32, 64)   36928       dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DTran (None, 64, 64, 32)   8224        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 64, 64, 64)   0           conv2d_transpose_10[0][0]        \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 64, 64, 32)   0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 64, 64, 32)   9248        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 128, 128, 16) 2064        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128, 128, 32) 0           conv2d_transpose_11[0][0]        \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 128, 128, 16) 0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 128, 128, 16) 2320        dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 128, 128, 1)  17          conv2d_55[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,940,817\n",
      "Trainable params: 1,940,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(IMG_WIDTH, IMG_HEIGHT, 1), batch_size=None)\n",
    "\n",
    "# Hier werden Preprocessing-Schritte ausgeführt\n",
    "# s ist hier dann Differnzbild (Pixelraum_Fourier)\n",
    "s = Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "\n",
    "# Contraction path\n",
    "c1 = Conv2D(\n",
    "    16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "c1 = Dropout(0.1)(c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c1)\n",
    "p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p1)\n",
    "c2 = Dropout(0.1)(c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c2)\n",
    "p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p2)\n",
    "c3 = Dropout(0.2)(c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c3)\n",
    "p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p3)\n",
    "c4 = Dropout(0.2)(c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "c5 = Conv2D(256, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p4)\n",
    "c5 = Dropout(0.3)(c5)\n",
    "c5 = Conv2D(256, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "# Expansive path\n",
    "u6 = Conv2DTranspose(\n",
    "    128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(u6)\n",
    "c6 = Dropout(0.2)(c6)\n",
    "c6 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "u7 = Conv2DTranspose(\n",
    "    64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(64, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(u7)\n",
    "c7 = Dropout(0.2)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "u8 = tf.keras.layers.Conv2DTranspose(\n",
    "    32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(u8)\n",
    "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "u9 = tf.keras.layers.Conv2DTranspose(\n",
    "    16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(u9)\n",
    "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "model_u_net = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "model_u_net.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_inv_fourier_trans(img):\n",
    "    return tf.math.round(tf.math.real(tf.signal.ifft2d(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppm: postprocessing model\n",
    "\n",
    "def create_postprocessing_model():\n",
    "\n",
    "    ppm_input_img_clean = tf.keras.Input(shape=(128, 128, 3), batch_size=1)\n",
    "    ppm_input_img_filmed = tf.keras.Input(shape=(128, 128, 3), batch_size=1)\n",
    "    ppm_input_unet_output = tf.keras.Input(shape=(128, 128), batch_size=1)\n",
    "\n",
    "    ppm_input_img_clean_complex_r = tf.keras.Input(\n",
    "        shape=(128, 128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_clean_complex_g = tf.keras.Input(\n",
    "        shape=(128, 128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_clean_complex_b = tf.keras.Input(\n",
    "        shape=(128, 128), batch_size=1, dtype=tf.complex64)\n",
    "\n",
    "    ppm_input_img_filmed_complex_r = tf.keras.Input(\n",
    "        shape=(128, 128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_filmed_complex_g = tf.keras.Input(\n",
    "        shape=(128, 128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_filmed_complex_b = tf.keras.Input(\n",
    "        shape=(128, 128), batch_size=1, dtype=tf.complex64)\n",
    "\n",
    "    ones = tf.ones((128, 128))\n",
    "    zeros = tf.zeros((128))\n",
    "    ones_t2c = tf.complex(ones, zeros)\n",
    "    u_net_output_t2c = tf.complex(ppm_input_unet_output, zeros)\n",
    "\n",
    "    # FORMEL: (1 - out) * clean + out * filmed\n",
    "    # out == mask\n",
    "    # t2c -> transfered to complex\n",
    "\n",
    "    # # NUR ZUM TESTEN:\n",
    "    # u_net_output_t2c = tf.complex(ones, zeros)\n",
    "\n",
    "    def soft_blending(clean, filmed, ones_t2c=ones_t2c, u_net_output_t2c=u_net_output_t2c):\n",
    "        zw1 = tf.math.subtract(ones_t2c, u_net_output_t2c)\n",
    "        zw1 = tf.math.multiply(zw1, clean)\n",
    "\n",
    "        zw2 = tf.multiply(u_net_output_t2c, filmed)\n",
    "        return tf.math.add(zw1, zw2)\n",
    "\n",
    "    img_processed_complex_fourier_r = soft_blending(\n",
    "        ppm_input_img_clean_complex_r, ppm_input_img_filmed_complex_r)\n",
    "\n",
    "    img_processed_complex_fourier_g = soft_blending(\n",
    "        ppm_input_img_clean_complex_g, ppm_input_img_filmed_complex_g)\n",
    "\n",
    "    img_processed_complex_fourier_b = soft_blending(\n",
    "        ppm_input_img_clean_complex_b, ppm_input_img_filmed_complex_b)\n",
    "\n",
    "    # # img_processed_px_fourier = tf.math.log(                     # zum anschauen\n",
    "    # #     tf.math.abs(img_processed_complex_fourier_r))\n",
    "\n",
    "    # ----------- INVERSE FOURIER TRANSFORMATION -----------\n",
    "\n",
    "    img_processed_r = tf_inv_fourier_trans(\n",
    "        img_processed_complex_fourier_r\n",
    "    ) / 255\n",
    "    # img_processed_r = tf.math.divide(img_processed_r, 255)\n",
    "\n",
    "    img_processed_g = tf_inv_fourier_trans(\n",
    "        img_processed_complex_fourier_g\n",
    "    ) / 255\n",
    "    # img_processed_g = tf.math.divide(img_processed_g, 255)\n",
    "\n",
    "    img_processed_b = tf_inv_fourier_trans(\n",
    "        img_processed_complex_fourier_b\n",
    "    ) / 255\n",
    "    # img_processed_b = tf.math.divide(img_processed_b, 255)\n",
    "\n",
    "    # HIERHER\n",
    "    # 3 Einzelkanäle zu einem RGB Bild\n",
    "    # img_processed_r = tf.math.log(tf.math.abs(img_processed_r)) #/255\n",
    "    # img_processed_g = tf.math.log(tf.math.abs(img_processed_g)) #/255\n",
    "    # img_processed_b = tf.math.log(tf.math.abs(img_processed_b)) #/255\n",
    "\n",
    "    img_processed_rgb = tf.stack(\n",
    "        [img_processed_r, img_processed_g, img_processed_b], axis=-1)\n",
    "\n",
    "    postprocess_model = tf.keras.Model(inputs=[\n",
    "        ppm_input_img_clean,\n",
    "        ppm_input_img_filmed,\n",
    "        ppm_input_unet_output,\n",
    "        ppm_input_img_clean_complex_r,\n",
    "        ppm_input_img_clean_complex_g,\n",
    "        ppm_input_img_clean_complex_b,\n",
    "        ppm_input_img_filmed_complex_r,\n",
    "        ppm_input_img_filmed_complex_g,\n",
    "        ppm_input_img_filmed_complex_b\n",
    "    ], outputs=[img_processed_rgb, img_processed_r], name=\"postprocessing_model\")\n",
    "\n",
    "    # postprocess_model.summary()\n",
    "    return postprocess_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_postprocessing_model():\n",
    "#     clean = tf.keras.Input(shape=(128, 128, 3), batch_size=1)\n",
    "#     darker = tf.math.multiply(clean, 3)\n",
    "\n",
    "#     postprocessing_model = tf.keras.Model(inputs=[clean], outputs=[darker])\n",
    "\n",
    "#     return postprocessing_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"postprocessing_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.dtypes.complex_2 (TFOpLambda (1, 128, 128)        0           input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_6 (TFOpLambda) (1, 128, 128)        0           tf.dtypes.complex_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_7 (TFOpLambda) (1, 128, 128)        0           tf.dtypes.complex_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_8 (TFOpLambda) (1, 128, 128)        0           tf.dtypes.complex_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_12 (TFOpLambda (1, 128, 128)        0           tf.math.subtract_6[0][0]         \n",
      "                                                                 input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_13 (TFOpLambda (1, 128, 128)        0           tf.dtypes.complex_2[0][0]        \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_14 (TFOpLambda (1, 128, 128)        0           tf.math.subtract_7[0][0]         \n",
      "                                                                 input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_15 (TFOpLambda (1, 128, 128)        0           tf.dtypes.complex_2[0][0]        \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_16 (TFOpLambda (1, 128, 128)        0           tf.math.subtract_8[0][0]         \n",
      "                                                                 input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_17 (TFOpLambda (1, 128, 128)        0           tf.dtypes.complex_2[0][0]        \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_6 (TFOpLambda)      (1, 128, 128)        0           tf.math.multiply_12[0][0]        \n",
      "                                                                 tf.math.multiply_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_7 (TFOpLambda)      (1, 128, 128)        0           tf.math.multiply_14[0][0]        \n",
      "                                                                 tf.math.multiply_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_8 (TFOpLambda)      (1, 128, 128)        0           tf.math.multiply_16[0][0]        \n",
      "                                                                 tf.math.multiply_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.signal.ifft2d_6 (TFOpLambda) (1, 128, 128)        0           tf.math.add_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.signal.ifft2d_7 (TFOpLambda) (1, 128, 128)        0           tf.math.add_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.signal.ifft2d_8 (TFOpLambda) (1, 128, 128)        0           tf.math.add_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.real_6 (TFOpLambda)     (1, 128, 128)        0           tf.signal.ifft2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.real_7 (TFOpLambda)     (1, 128, 128)        0           tf.signal.ifft2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.real_8 (TFOpLambda)     (1, 128, 128)        0           tf.signal.ifft2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.round_6 (TFOpLambda)    (1, 128, 128)        0           tf.math.real_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.round_7 (TFOpLambda)    (1, 128, 128)        0           tf.math.real_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.round_8 (TFOpLambda)    (1, 128, 128)        0           tf.math.real_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(1, 128, 128, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(1, 128, 128, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.stack_2 (TFOpLambda)         (1, 128, 128, 3)     0           tf.math.round_6[0][0]            \n",
      "                                                                 tf.math.round_7[0][0]            \n",
      "                                                                 tf.math.round_8[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "postprocessing_model = create_postprocessing_model()\n",
    "postprocessing_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Train loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_mean_alias_value_of_one_image(x_filmed, x_clean):\n",
    "    # print(\"betrete calc_mean_aias_value_of_one_image\")\n",
    "\n",
    "    # print(\"x_filmed: \")\n",
    "    # plt.imshow(x_filmed)\n",
    "    # plt.show()\n",
    "    # print(\"x_clean: \")\n",
    "    # plt.imshow(x_clean)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    fourier_handler = Fourier_Images(x_filmed, x_clean)\n",
    "\n",
    "    img_filmed_r, img_filmed_g, img_filmed_b = fourier_handler.split_RGB_2_Grayscale(\n",
    "        x_filmed)\n",
    "    img_clean_r, img_clean_g, img_clean_b = fourier_handler.split_RGB_2_Grayscale(\n",
    "        x_clean)\n",
    "\n",
    "    img_filmed_complex_r = tf.signal.fft2d(img_filmed_r)\n",
    "    img_filmed_complex_g = tf.signal.fft2d(img_filmed_g)\n",
    "    img_filmed_complex_b = tf.signal.fft2d(img_filmed_b)\n",
    "\n",
    "    img_clean_complex_r = tf.signal.fft2d(img_clean_r)\n",
    "    img_clean_complex_g = tf.signal.fft2d(img_clean_g)\n",
    "    img_clean_complex_b = tf.signal.fft2d(img_clean_b)\n",
    "\n",
    "\n",
    "    x_filmed_fourier_px, x_clean_fourier_px, differenzbild_fourier_px, img_filmed_fourier_combined, img_clean_fourier_combined = fourier_handler.generate_mask_from_images(\n",
    "        img_filmed_complex_r, img_filmed_complex_g, img_filmed_complex_b, img_clean_complex_r, img_clean_complex_g,  img_clean_complex_b\n",
    "    )\n",
    "    # plt.imsave(\".\\\\tmp\\\\img_filmed_fourier_combined.png\",\n",
    "    #            img_filmed_fourier_combined, cmap=\"gray\")\n",
    "    # plt.imsave(\".\\\\tmp\\\\img_clean_fourier_combined.png\",\n",
    "    #            img_clean_fourier_combined, cmap=\"gray\")\n",
    "    if show_intermediate_pics:\n",
    "        plt.imshow(differenzbild_fourier_px, cmap=\"gray\")\n",
    "        plt.show()\n",
    "\n",
    "    differenzbild_fourier_px = differenzbild_fourier_px.reshape(\n",
    "        1, IMG_WIDTH, IMG_HEIGHT, 1)    \n",
    "\n",
    "\n",
    "\n",
    "    del(fourier_handler)\n",
    "\n",
    "\n",
    "\n",
    "    # ----------- HIER STARTET U-NET-MODEL -----------\n",
    "\n",
    "    u_net_output = execute_UNet_model(\n",
    "        differenzbild_fourier_px, training=True)\n",
    "\n",
    "\n",
    "    # ----------- ALPHA-BLENDING -----------\n",
    "\n",
    "    # FORMEL: (1 - out) * clean + out * filmed\n",
    "    # t2c -> transfered to complex\n",
    "\n",
    "\n",
    "    if show_intermediate_pics:\n",
    "        print(\"x_clean: \")\n",
    "        plt.imshow(x_clean)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"x_filmed: \")\n",
    "        plt.imshow(x_filmed)\n",
    "        plt.show()\n",
    "\n",
    "    x_clean = x_clean.reshape((1, IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    x_filmed = x_filmed.reshape((1, IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "\n",
    "\n",
    "    u_net_output = tf.reshape(\n",
    "        u_net_output, shape=(IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "    if show_intermediate_pics:\n",
    "        print(\"u_net_output orig: \")\n",
    "        plt.imshow(u_net_output, cmap=\"gray\")\n",
    "        plt.show()\n",
    "\n",
    "    # u_net_output = np.zeros((IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "    # if show_intermediate_pics:\n",
    "    #     print(\"u_net_output test: \")\n",
    "    #     plt.imshow(u_net_output, cmap=\"gray\")\n",
    "    #     plt.show()\n",
    "\n",
    "\n",
    "    img_clean_complex_r = tf.reshape(img_clean_complex_r, (1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_clean_complex_g = tf.reshape(img_clean_complex_g, (1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_clean_complex_b = tf.reshape(img_clean_complex_b, (1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_filmed_complex_r = tf.reshape(img_filmed_complex_r, (1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_filmed_complex_g = tf.reshape(img_filmed_complex_g, (1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_filmed_complex_b = tf.reshape(img_filmed_complex_b, (1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "\n",
    "\n",
    "    # ------------ postprocessing Model aufrufen --------\n",
    "\n",
    "    # HIERHER\n",
    "\n",
    "    image_processed_rgb, img_processed_r = execute_postprocessing_model([\n",
    "        x_clean ,\n",
    "        x_filmed ,\n",
    "        u_net_output,\n",
    "        img_clean_complex_r ,\n",
    "        img_clean_complex_g ,\n",
    "        img_clean_complex_b ,\n",
    "        img_filmed_complex_r ,\n",
    "        img_filmed_complex_g ,\n",
    "        img_filmed_complex_b ,\n",
    "    ])\n",
    "\n",
    "\n",
    "    # tmp = img_processed_r\n",
    "    # tmp = np.array(tmp).reshape(IMG_WIDTH, IMG_HEIGHT)\n",
    "    # print(\"-------------------------------> img_processed_r: \")\n",
    "    # plt.imshow(tmp, cmap=\"gray\")\n",
    "    # plt.show()\n",
    "\n",
    "    if show_intermediate_pics:\n",
    "        print(\"image_processed_rgb: \")\n",
    "        # print(np.array(image_processed_rgb).reshape(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "        plt.imshow(np.array(image_processed_rgb).reshape(\n",
    "            IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # ----------- Feed multiple Buckets in CNN for predicting amount of alias -----------\n",
    "\n",
    "    def create_multiple_buckets_to_feed_them_into_CNN(tensor):\n",
    "\n",
    "        BATCH_SIZE_CNN = 1\n",
    "        NUM_BOXES = 30\n",
    "        CROP_SIZE = (60, 60)\n",
    "\n",
    "        boxes = tf.random.uniform(shape=(NUM_BOXES, 4))\n",
    "        box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0,\n",
    "                                        maxval=BATCH_SIZE_CNN, dtype=tf.int32)\n",
    "        \n",
    "        output = tf.image.crop_and_resize(\n",
    "            tensor, boxes, box_indices, CROP_SIZE)\n",
    "        return output\n",
    "\n",
    "    cnn_input = create_multiple_buckets_to_feed_them_into_CNN(image_processed_rgb)\n",
    "\n",
    "    # print(\"cnn_input: \")\n",
    "    # print(cnn_input)\n",
    "\n",
    "\n",
    "\n",
    "    # ------- Make prediction --------\n",
    "\n",
    "    y_pred = execute_cnn_model(cnn_input)\n",
    "\n",
    "    y_pred = tf.math.reduce_mean(\n",
    "        y_pred, keepdims=True\n",
    "    )\n",
    "\n",
    "    # print(\"Ende von calc_mean_aias_value_of_one_image:\")\n",
    "    # print(\"y_pred: \")\n",
    "    # print(y_pred)\n",
    "\n",
    "    return y_pred, differenzbild_fourier_px, u_net_output, image_processed_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|                                   | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next batch...\n",
      "WARNING:tensorflow:Model was constructed with shape (1, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(1, 128, 128), dtype=tf.float32, name='input_24'), name='input_24', description=\"created by layer 'input_24'\"), but it was called on an input with incompatible shape (128, 128).\n",
      "y_pred: \n",
      "tf.Tensor([[212.06729]], shape=(1, 1), dtype=float32)\n",
      "y_pred: \n",
      "tf.Tensor([[202.69434]], shape=(1, 1), dtype=float32)\n",
      "y_pred: \n",
      "tf.Tensor([[208.19127]], shape=(1, 1), dtype=float32)\n",
      "y_pred: \n",
      "tf.Tensor([[199.66559]], shape=(1, 1), dtype=float32)\n",
      "y_pred: \n",
      "tf.Tensor([[213.40279]], shape=(1, 1), dtype=float32)\n",
      "cnn_mean_prediction_values.shape: \n",
      "(5, 1, 1)\n",
      "loss: \n",
      "tf.Tensor(207.20425, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|                                   | 0/5 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients: \n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['conv2d_38/kernel:0', 'conv2d_38/bias:0', 'conv2d_39/kernel:0', 'conv2d_39/bias:0', 'conv2d_40/kernel:0', 'conv2d_40/bias:0', 'conv2d_41/kernel:0', 'conv2d_41/bias:0', 'conv2d_42/kernel:0', 'conv2d_42/bias:0', 'conv2d_43/kernel:0', 'conv2d_43/bias:0', 'conv2d_44/kernel:0', 'conv2d_44/bias:0', 'conv2d_45/kernel:0', 'conv2d_45/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_transpose_8/kernel:0', 'conv2d_transpose_8/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_transpose_9/kernel:0', 'conv2d_transpose_9/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'conv2d_transpose_10/kernel:0', 'conv2d_transpose_10/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_transpose_11/kernel:0', 'conv2d_transpose_11/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Main\\MA_PROGR\\Code_Fourier\\UNet_for_Fourier.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=121'>122</a>\u001b[0m     \u001b[39mprint\u001b[39m(gradients)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=123'>124</a>\u001b[0m     \u001b[39m# Optimize the model:\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=124'>125</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mapply_gradients(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=125'>126</a>\u001b[0m         \u001b[39mzip\u001b[39;49m(gradients, model_u_net\u001b[39m.\u001b[39;49mtrainable_variables))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=127'>128</a>\u001b[0m     tmp \u001b[39m=\u001b[39m epoch_loss\u001b[39m.\u001b[39mupdate_state(y_true, y_pred)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=129'>130</a>\u001b[0m     \u001b[39m# print(\"tmp\")\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=130'>131</a>\u001b[0m     \u001b[39m# print(tmp)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=131'>132</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=149'>150</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000023?line=150'>151</a>\u001b[0m \u001b[39m# End Epoch\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programme\\Anaconda3\\envs\\tf_training\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:630\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=588'>589</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_gradients\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=589'>590</a>\u001b[0m                     grads_and_vars,\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=590'>591</a>\u001b[0m                     name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=591'>592</a>\u001b[0m                     experimental_aggregate_gradients\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=592'>593</a>\u001b[0m   \u001b[39m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=593'>594</a>\u001b[0m \n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=594'>595</a>\u001b[0m \u001b[39m  This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=627'>628</a>\u001b[0m \u001b[39m    RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=628'>629</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=629'>630</a>\u001b[0m   grads_and_vars \u001b[39m=\u001b[39m optimizer_utils\u001b[39m.\u001b[39;49mfilter_empty_gradients(grads_and_vars)\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=630'>631</a>\u001b[0m   var_list \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m (_, v) \u001b[39min\u001b[39;00m grads_and_vars]\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=632'>633</a>\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope_v2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name):\n\u001b[0;32m    <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py?line=633'>634</a>\u001b[0m     \u001b[39m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programme\\Anaconda3\\envs\\tf_training\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\utils.py:75\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/utils.py?line=71'>72</a>\u001b[0m filtered \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(filtered)\n\u001b[0;32m     <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/utils.py?line=73'>74</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered:\n\u001b[1;32m---> <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/utils.py?line=74'>75</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m     <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/utils.py?line=75'>76</a>\u001b[0m                    ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars],))\n\u001b[0;32m     <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/utils.py?line=76'>77</a>\u001b[0m \u001b[39mif\u001b[39;00m vars_with_empty_grads:\n\u001b[0;32m     <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/utils.py?line=77'>78</a>\u001b[0m   logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/utils.py?line=78'>79</a>\u001b[0m       (\u001b[39m\"\u001b[39m\u001b[39mGradients do not exist for variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m when minimizing the loss.\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     <a href='file:///d%3A/Programme/Anaconda3/envs/tf_training/lib/site-packages/tensorflow/python/keras/optimizer_v2/utils.py?line=79'>80</a>\u001b[0m       ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vars_with_empty_grads]))\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['conv2d_38/kernel:0', 'conv2d_38/bias:0', 'conv2d_39/kernel:0', 'conv2d_39/bias:0', 'conv2d_40/kernel:0', 'conv2d_40/bias:0', 'conv2d_41/kernel:0', 'conv2d_41/bias:0', 'conv2d_42/kernel:0', 'conv2d_42/bias:0', 'conv2d_43/kernel:0', 'conv2d_43/bias:0', 'conv2d_44/kernel:0', 'conv2d_44/bias:0', 'conv2d_45/kernel:0', 'conv2d_45/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_47/kernel:0', 'conv2d_47/bias:0', 'conv2d_transpose_8/kernel:0', 'conv2d_transpose_8/bias:0', 'conv2d_48/kernel:0', 'conv2d_48/bias:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_transpose_9/kernel:0', 'conv2d_transpose_9/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'conv2d_51/kernel:0', 'conv2d_51/bias:0', 'conv2d_transpose_10/kernel:0', 'conv2d_transpose_10/bias:0', 'conv2d_52/kernel:0', 'conv2d_52/bias:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_transpose_11/kernel:0', 'conv2d_transpose_11/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_55/kernel:0', 'conv2d_55/bias:0', 'conv2d_56/kernel:0', 'conv2d_56/bias:0']."
     ]
    }
   ],
   "source": [
    "# Custom train loop\n",
    "\n",
    "SAFE_RESULTS = True\n",
    "WRITE_PATH = \"D:\\\\Main\\\\MA_PROGR\\\\Data\\\\UNET_Output\\\\Session06\"\n",
    "\n",
    "show_intermediate_pics = False\n",
    "\n",
    "RGB_WEIGHTS = [0.299, 0.587, 0.114]\n",
    "\n",
    "\n",
    "overall_train_loss = []\n",
    "\n",
    "\n",
    "execute_UNet_model = tf.function(model_u_net)\n",
    "execute_cnn_model = tf.function(model_cnn)\n",
    "execute_postprocessing_model = tf.function(postprocessing_model)\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "batch_size_cnn = 1\n",
    "batch_size_unet = 5\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "epoch_loss = tf.keras.metrics.MeanAbsoluteError()\n",
    "# epoch_loss = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "# epoch_loss = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_writer = tf.summary.create_file_writer(\"u_net_logs/train/\")\n",
    "test_writer = tf.summary.create_file_writer(\"u_net_logs/test/\")\n",
    "\n",
    "y_true = tf.constant(0, dtype=tf.float16, name=\"y_true\")\n",
    "\n",
    "jump_to_new_epoch = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print(f\"Start of epoch {epoch}\")\n",
    "    jump_to_new_epoch = False\n",
    "\n",
    "    # Mache neue Generater, sodass wieder durch alle Bilder durchgegange wird für die nächste Epoche:\n",
    "    train_filmed_img_gen_obj = my_train_filmed_gen()\n",
    "    train_clean_img_gen_obj = my_train_clean_gen()\n",
    "\n",
    "    # ---------------------------------------------------------------------------- BATCHES SAMMELN Start ----------------------------------------------------------------------------------------\n",
    "    train_step = 0\n",
    "\n",
    "    for batch_idx in tqdm(range(batch_size_unet), desc=\"training...\", ascii=False, ncols=75):\n",
    "        print(\"next batch...\")\n",
    "        cnn_mean_prediction_values = []\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "        \n",
    "            # Bilder für den nächsten Batch sammeln:\n",
    "            for i in range(batch_size_unet):\n",
    "                try:\n",
    "                    x_filmed = train_filmed_img_gen_obj.__next__()\n",
    "                    x_clean = train_clean_img_gen_obj.__next__()\n",
    "                except StopIteration as e:\n",
    "                    print(\"------------------------Am Ende angelangt, gehe in neue Epoche\")\n",
    "\n",
    "                    jump_to_new_epoch = True\n",
    "\n",
    "                if jump_to_new_epoch == True:\n",
    "                    # Alle Bilder sind aufgebraucht, gehe in neue Epoche\n",
    "                    break\n",
    "\n",
    "                \n",
    "                y_pred, differenzbild_fourier_px, u_net_output, image_processed_rgb = calc_mean_alias_value_of_one_image(x_filmed, x_clean)\n",
    "                cnn_mean_prediction_values.append(y_pred)\n",
    "\n",
    "                print(\"y_pred: \")\n",
    "                print(y_pred)\n",
    "\n",
    "                # ---------------------------------------------------------------------------- BATCHES SAMMELN ENDE ----------------------------------------------------------------------------------------\n",
    "\n",
    "            \n",
    "            if SAFE_RESULTS:\n",
    "                plt.imsave(f\"{WRITE_PATH}\\\\x_filmed_{epoch}_{batch_idx}.png\", x_filmed)\n",
    "\n",
    "                plt.imsave(f\"{WRITE_PATH}\\\\differenzbild_{epoch}_{batch_idx}.png\",\n",
    "                   differenzbild_fourier_px.reshape((IMG_WIDTH,IMG_HEIGHT)), cmap=\"gray\")\n",
    "\n",
    "                plt.imsave(f\"{WRITE_PATH}\\\\unet_output_{epoch}_{batch_idx}.png\",\n",
    "                   u_net_output, cmap=\"gray\")\n",
    "\n",
    "\n",
    "                # Umformen in Numpy-Array\n",
    "                image_processed_rgb = np.array(image_processed_rgb).reshape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "                # Max-Wert liegt manchesmal über 1 (keine ahnung warum...)\n",
    "                if image_processed_rgb.max() >= 1:\n",
    "                    # Werte auf 0 - 1 bringen\n",
    "                    image_processed_rgb = image_processed_rgb / image_processed_rgb.max()\n",
    "\n",
    "                # print(\"image_processed_rgb.max():\")\n",
    "                # print(image_processed_rgb.max())\n",
    "                plt.imsave(f\"{WRITE_PATH}\\\\image_processed_rgb_{epoch}_{batch_idx}.png\",\n",
    "                   np.array(image_processed_rgb).reshape((IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "\n",
    "            if jump_to_new_epoch == True:\n",
    "                # Alle Bilder sind aufgebraucht, gehe in neue epoche\n",
    "                break\n",
    "\n",
    "            # ----------- Calc loss -----------\n",
    "\n",
    "            y_true = tf.zeros(batch_size_unet, 1)\n",
    "\n",
    "            print(\"cnn_mean_prediction_values.shape: \")\n",
    "            print( np.array(cnn_mean_prediction_values).shape)\n",
    "\n",
    "            loss = loss_fn(y_true=y_true, y_pred=cnn_mean_prediction_values)\n",
    "            print(\"loss: \")\n",
    "            print(loss)\n",
    "\n",
    "            gradients = tape.gradient(loss, model_u_net.trainable_weights)\n",
    "            del cnn_mean_prediction_values[:]\n",
    "            print(\"gradients: \")\n",
    "            print(gradients)\n",
    "\n",
    "            # Optimize the model:\n",
    "            optimizer.apply_gradients(\n",
    "                zip(gradients, model_u_net.trainable_variables))\n",
    "\n",
    "            tmp = epoch_loss.update_state(y_true, y_pred)\n",
    "\n",
    "            # print(\"tmp\")\n",
    "            # print(tmp)\n",
    "\n",
    "\n",
    "            # print(\"------------------------------------\")\n",
    "            # print(\"y_true:\")\n",
    "            # print(y_true)\n",
    "\n",
    "            # print(\"y_pred\")\n",
    "            # print(y_pred)\n",
    "            # print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "            # with train_writer.as_default():\n",
    "            #     tf.summary.scalar(\"Loss\", loss, step=train_step)\n",
    "            #     tf.summary.scalar(\n",
    "            #         \"Accuracy\", epoch_loss.result(), step=train_step,\n",
    "            #     )\n",
    "            # train_step += 1\n",
    "        \n",
    "        # End Epoch\n",
    "\n",
    "        print(f\"--------------- epoch_loss: {epoch_loss.result()}\")\n",
    "        overall_train_loss.append(epoch_loss.result())\n",
    "\n",
    "\n",
    "\n",
    "# overall_train_loss = epoch_loss.result()\n",
    "# print(f\"overall_train_loss: {overall_train_loss}\")\n",
    "# epoch_loss.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x208019e4ca0>]"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(overall_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(overall_train_loss)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Main\\MA_PROGR\\Code_Fourier\\UNet_for_Fourier.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000026?line=0'>1</a>\u001b[0m test_img \u001b[39m=\u001b[39m img[\u001b[39m400\u001b[39m:\u001b[39m528\u001b[39m, \u001b[39m400\u001b[39m:\u001b[39m528\u001b[39m,:]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000026?line=1'>2</a>\u001b[0m test_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(test_img)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m , \u001b[39m128\u001b[39m , \u001b[39m3\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000026?line=3'>4</a>\u001b[0m \u001b[39m# plt.imshow(test_img)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "test_img = img[400:528, 400:528,:]\n",
    "test_img = np.array(test_img).reshape(1, 128 , 128 , 3)\n",
    "\n",
    "# plt.imshow(test_img)\n",
    "\n",
    "test_img= tf.convert_to_tensor(test_img, dtype=tf.float32)\n",
    "\n",
    "test_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22300133f10>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "BATCH_SIZE = 1\n",
    "NUM_BOXES = 3\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "CHANNELS = 3\n",
    "CROP_SIZE = (24, 24)\n",
    "\n",
    "test_img = img[400:528, 400:528, :]\n",
    "test_img = np.array(test_img).reshape(1, 128, 128, 3)\n",
    "\n",
    "# plt.imshow(test_img)\n",
    "\n",
    "test_img = tf.convert_to_tensor(test_img, dtype=tf.float32)\n",
    "image = test_img\n",
    "boxes = tf.random.uniform(shape=(NUM_BOXES, 4))\n",
    "# plt.imshow(np.array(image).reshape(IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\n",
    "# plt.show()\n",
    "box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0,\n",
    "                                maxval=BATCH_SIZE, dtype=tf.int32)\n",
    "output = tf.image.crop_and_resize(image, boxes, box_indices, CROP_SIZE)\n",
    "output.shape  # => (5, 24, 24, 3)\n",
    "\n",
    "plt.imshow(output[2])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe6f1f0ffae94815f8b68c5970304948992b116503ec65631f55d524065bfaa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf_training')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
