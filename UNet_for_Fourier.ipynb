{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import  InputLayer, Conv2D, Lambda, Dropout, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from UNet_Fourier_Facilities import Fourier_Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = tf.keras.models.load_model(\"./cnn_models/single_rgb_image_regression_V02_epochs_100_1653595623\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cnn = tf.keras.models.load_model(\"./models/single_rgb_image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img1 = Image.open(\n",
    "#     \"D:\\Main\\MA_PROGR\\Data\\Train\\LED_Wand_Aufnahmen\\Alias\\LED_Wand_20001.png\")\n",
    "# img1 = np.asarray(img1)/255\n",
    "# img1 = img1.reshape(1, 60, 60, 3)\n",
    "# res1 = model.predict(img1)\n",
    "# print(res1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILMED_PATH = \"D:\\\\Main\\\\MA_PROGR\\\\Data\\\\Train\\\\UNet_Train\\\\model_at_tree_100_pics\\\\filmed\"\n",
    "TRAIN_CLEAN_PATH = \"D:\\\\Main\\\\MA_PROGR\\\\Data\\\\Train\\\\UNet_Train\\\\model_at_tree_100_pics\\\\clean_aligned\"\n",
    "# TEST_PATH = \"./Data/data-science-bowl-2018/stage1_test/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filmed_imgs = []\n",
    "train_clean_imgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSET = 200\n",
    "abbruch_idx = 30\n",
    "\n",
    "\n",
    "def my_train_filmed_gen():\n",
    "    for i, addr_filmed in enumerate(os.listdir(TRAIN_FILMED_PATH)):\n",
    "        img =  plt.imread(f\"{TRAIN_FILMED_PATH}\\{addr_filmed}\")\n",
    "        yield img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3]\n",
    "\n",
    "def my_train_clean_gen():\n",
    "    for i, addr_clean in enumerate(os.listdir(TRAIN_CLEAN_PATH)):\n",
    "        img =  plt.imread(f\"{TRAIN_CLEAN_PATH}\\{addr_clean}\")\n",
    "        yield img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3]\n",
    "\n",
    "\n",
    "train_filmed_img_gen_obj = my_train_filmed_gen()\n",
    "train_clean_img_gen_obj = my_train_clean_gen()\n",
    "\n",
    "# for i, addr_filmed in enumerate( os.listdir(TRAIN_FILMED_PATH)):\n",
    "#     if i > 5:\n",
    "#         break\n",
    "#     img = plt.imread(f\"{TRAIN_FILMED_PATH}\\{addr_filmed}\")\n",
    "\n",
    "#     # :3 falls es Alpha Channel gibt (der soll weg)\n",
    "#     train_filmed_imgs.append(\n",
    "#         img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3])\n",
    "\n",
    "# for i , addr_clean in enumerate(os.listdir(TRAIN_CLEAN_PATH)):\n",
    "#     if i > 5:\n",
    "#         break\n",
    "#     # :3 falls es Alpha Channel gibt (der soll weg)\n",
    "#     img = plt.imread(f\"{TRAIN_CLEAN_PATH}\\{addr_clean}\")\n",
    "#     train_clean_imgs.append(\n",
    "#         img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_filmed_img_gen_obj.__next__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_clean_img_gen_obj.__next__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_clean_imgs[0])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build U-Net-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_241 (InputLayer)          [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 128, 128, 1)  0           input_241[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_456 (Conv2D)             (None, 128, 128, 16) 160         lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_216 (Dropout)           (None, 128, 128, 16) 0           conv2d_456[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_457 (Conv2D)             (None, 128, 128, 16) 2320        dropout_216[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_96 (MaxPooling2D) (None, 64, 64, 16)   0           conv2d_457[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_458 (Conv2D)             (None, 64, 64, 32)   4640        max_pooling2d_96[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_217 (Dropout)           (None, 64, 64, 32)   0           conv2d_458[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_459 (Conv2D)             (None, 64, 64, 32)   9248        dropout_217[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_97 (MaxPooling2D) (None, 32, 32, 32)   0           conv2d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_460 (Conv2D)             (None, 32, 32, 64)   18496       max_pooling2d_97[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_218 (Dropout)           (None, 32, 32, 64)   0           conv2d_460[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_461 (Conv2D)             (None, 32, 32, 64)   36928       dropout_218[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_98 (MaxPooling2D) (None, 16, 16, 64)   0           conv2d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_462 (Conv2D)             (None, 16, 16, 128)  73856       max_pooling2d_98[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_219 (Dropout)           (None, 16, 16, 128)  0           conv2d_462[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_463 (Conv2D)             (None, 16, 16, 128)  147584      dropout_219[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_99 (MaxPooling2D) (None, 8, 8, 128)    0           conv2d_463[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_464 (Conv2D)             (None, 8, 8, 256)    295168      max_pooling2d_99[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_220 (Dropout)           (None, 8, 8, 256)    0           conv2d_464[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_465 (Conv2D)             (None, 8, 8, 256)    590080      dropout_220[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_96 (Conv2DTran (None, 16, 16, 128)  131200      conv2d_465[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 16, 16, 256)  0           conv2d_transpose_96[0][0]        \n",
      "                                                                 conv2d_463[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_466 (Conv2D)             (None, 16, 16, 128)  295040      concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_221 (Dropout)           (None, 16, 16, 128)  0           conv2d_466[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_467 (Conv2D)             (None, 16, 16, 128)  147584      dropout_221[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_97 (Conv2DTran (None, 32, 32, 64)   32832       conv2d_467[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 32, 32, 128)  0           conv2d_transpose_97[0][0]        \n",
      "                                                                 conv2d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_468 (Conv2D)             (None, 32, 32, 64)   73792       concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_222 (Dropout)           (None, 32, 32, 64)   0           conv2d_468[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_469 (Conv2D)             (None, 32, 32, 64)   36928       dropout_222[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_98 (Conv2DTran (None, 64, 64, 32)   8224        conv2d_469[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 64, 64, 64)   0           conv2d_transpose_98[0][0]        \n",
      "                                                                 conv2d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_470 (Conv2D)             (None, 64, 64, 32)   18464       concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_223 (Dropout)           (None, 64, 64, 32)   0           conv2d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_471 (Conv2D)             (None, 64, 64, 32)   9248        dropout_223[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_99 (Conv2DTran (None, 128, 128, 16) 2064        conv2d_471[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_99 (Concatenate)    (None, 128, 128, 32) 0           conv2d_transpose_99[0][0]        \n",
      "                                                                 conv2d_457[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_472 (Conv2D)             (None, 128, 128, 16) 4624        concatenate_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_224 (Dropout)           (None, 128, 128, 16) 0           conv2d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_473 (Conv2D)             (None, 128, 128, 16) 2320        dropout_224[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_474 (Conv2D)             (None, 128, 128, 1)  17          conv2d_473[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,940,817\n",
      "Trainable params: 1,940,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(IMG_WIDTH, IMG_HEIGHT, 1), batch_size=None)\n",
    "\n",
    "# Hier werden Preprocessing-Schritte ausgeführt\n",
    "# s ist hier dann Differnzbild (Pixelraum_Fourier)\n",
    "s = Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "\n",
    "# Contraction path\n",
    "c1 = Conv2D(\n",
    "    16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "c1 = Dropout(0.1)(c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c1)\n",
    "p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p1)\n",
    "c2 = Dropout(0.1)(c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c2)\n",
    "p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p2)\n",
    "c3 = Dropout(0.2)(c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c3)\n",
    "p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p3)\n",
    "c4 = Dropout(0.2)(c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "c5 = Conv2D(256, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p4)\n",
    "c5 = Dropout(0.3)(c5)\n",
    "c5 = Conv2D(256, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "# Expansive path\n",
    "u6 = Conv2DTranspose(\n",
    "    128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(u6)\n",
    "c6 = Dropout(0.2)(c6)\n",
    "c6 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "u7 = Conv2DTranspose(\n",
    "    64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(64, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(u7)\n",
    "c7 = Dropout(0.2)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "u8 = tf.keras.layers.Conv2DTranspose(\n",
    "    32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(u8)\n",
    "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "u9 = tf.keras.layers.Conv2DTranspose(\n",
    "    16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(u9)\n",
    "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "model_u_net = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "model_u_net.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppm: postprocessing model \n",
    "\n",
    "def create_postprocessing_model():\n",
    "\n",
    "    ppm_input_img_clean = tf.keras.Input(shape=(128,128, 3), batch_size=1)\n",
    "    ppm_input_img_filmed = tf.keras.Input(shape=(128,128, 3), batch_size=1)\n",
    "    ppm_input_unet_output = tf.keras.Input(shape=(128,128), batch_size=1)\n",
    "\n",
    "    ppm_input_img_clean_complex_r = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_clean_complex_g = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_clean_complex_b = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "\n",
    "    ppm_input_img_filmed_complex_r = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_filmed_complex_g = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_filmed_complex_b = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "\n",
    "\n",
    "\n",
    "    ones = tf.ones((128, 128))\n",
    "    zeros = tf.zeros((128, 128))\n",
    "    ones_t2c = tf.complex(ones, zeros)\n",
    "    u_net_output_t2c = tf.complex(ppm_input_unet_output, zeros)\n",
    "\n",
    "    # FORMEL: (1 - out) * clean + out * filmed\n",
    "    # out == mask \n",
    "    # t2c -> transfered to complex\n",
    "\n",
    "\n",
    "    def soft_blending(clean, filmed, ones_t2c=ones_t2c, u_net_output_t2c=u_net_output_t2c):\n",
    "        zw1 = tf.math.subtract(ones_t2c, u_net_output_t2c)\n",
    "        zw1 = tf.math.multiply(zw1, clean)\n",
    "\n",
    "        zw2 = tf.multiply(u_net_output_t2c, filmed)\n",
    "        return tf.math.add(zw1, zw2)\n",
    "\n",
    "\n",
    "    img_processed_complex_fourier_r = soft_blending(\n",
    "        ppm_input_img_clean_complex_r, ppm_input_img_filmed_complex_r)\n",
    "\n",
    "    img_processed_complex_fourier_g = soft_blending(\n",
    "        ppm_input_img_clean_complex_g, ppm_input_img_filmed_complex_g)\n",
    "\n",
    "    img_processed_complex_fourier_b = soft_blending(\n",
    "        ppm_input_img_clean_complex_b, ppm_input_img_filmed_complex_b)\n",
    "\n",
    "\n",
    "\n",
    "    # # img_processed_px_fourier = tf.math.log(                     # zum anschauen\n",
    "    # #     tf.math.abs(img_processed_complex_fourier_r))\n",
    "\n",
    "    # ----------- INVERSE FOURIER TRANSFORMATION -----------\n",
    "\n",
    "    img_processed_r = tf.signal.ifft2d(\n",
    "        img_processed_complex_fourier_r\n",
    "    )\n",
    "    img_processed_g = tf.signal.ifft2d(\n",
    "        img_processed_complex_fourier_g\n",
    "    )\n",
    "    img_processed_b = tf.signal.ifft2d(\n",
    "        img_processed_complex_fourier_b\n",
    "    )\n",
    "\n",
    "\n",
    "    # 3 Einzelkanäle zu einem RGB Bild\n",
    "    img_processed_r = tf.math.abs(img_processed_r)/255\n",
    "    img_processed_g = tf.math.abs(img_processed_g)/255\n",
    "    img_processed_b = tf.math.abs(img_processed_b)/255\n",
    "\n",
    "    img_processed_rgb = tf.stack(\n",
    "        [img_processed_r, img_processed_g, img_processed_b], axis=3)\n",
    "\n",
    "    postprocess_model = tf.keras.Model(inputs=[\n",
    "        ppm_input_img_clean, \n",
    "        ppm_input_img_filmed,\n",
    "        ppm_input_unet_output,\n",
    "        ppm_input_img_clean_complex_r ,\n",
    "        ppm_input_img_clean_complex_g ,\n",
    "        ppm_input_img_clean_complex_b ,\n",
    "        ppm_input_img_filmed_complex_r, \n",
    "        ppm_input_img_filmed_complex_g,\n",
    "        ppm_input_img_filmed_complex_b\n",
    "    ], outputs=[img_processed_rgb], name=\"postprocessing_model\")\n",
    "\n",
    "    # postprocess_model.summary()\n",
    "    return postprocess_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_postprocessing_model():\n",
    "#     clean = tf.keras.Input(shape=(128, 128, 3), batch_size=1)\n",
    "#     darker = tf.math.multiply(clean, 3)\n",
    "\n",
    "#     postprocessing_model = tf.keras.Model(inputs=[clean], outputs=[darker])\n",
    "\n",
    "#     return postprocessing_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"postprocessing_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_244 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.dtypes.complex_24 (TFOpLambd (1, 128, 128)        0           input_244[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_72 (TFOpLambda (1, 128, 128)        0           tf.dtypes.complex_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_245 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_248 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_73 (TFOpLambda (1, 128, 128)        0           tf.dtypes.complex_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_246 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_249 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_74 (TFOpLambda (1, 128, 128)        0           tf.dtypes.complex_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_247 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_250 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_144 (TFOpLambd (1, 128, 128)        0           tf.math.subtract_72[0][0]        \n",
      "                                                                 input_245[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_145 (TFOpLambd (1, 128, 128)        0           tf.dtypes.complex_24[0][0]       \n",
      "                                                                 input_248[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_146 (TFOpLambd (1, 128, 128)        0           tf.math.subtract_73[0][0]        \n",
      "                                                                 input_246[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_147 (TFOpLambd (1, 128, 128)        0           tf.dtypes.complex_24[0][0]       \n",
      "                                                                 input_249[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_148 (TFOpLambd (1, 128, 128)        0           tf.math.subtract_74[0][0]        \n",
      "                                                                 input_247[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_149 (TFOpLambd (1, 128, 128)        0           tf.dtypes.complex_24[0][0]       \n",
      "                                                                 input_250[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_72 (TFOpLambda)     (1, 128, 128)        0           tf.math.multiply_144[0][0]       \n",
      "                                                                 tf.math.multiply_145[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_73 (TFOpLambda)     (1, 128, 128)        0           tf.math.multiply_146[0][0]       \n",
      "                                                                 tf.math.multiply_147[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_74 (TFOpLambda)     (1, 128, 128)        0           tf.math.multiply_148[0][0]       \n",
      "                                                                 tf.math.multiply_149[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.signal.ifft2d_72 (TFOpLambda (1, 128, 128)        0           tf.math.add_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.signal.ifft2d_73 (TFOpLambda (1, 128, 128)        0           tf.math.add_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.signal.ifft2d_74 (TFOpLambda (1, 128, 128)        0           tf.math.add_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_72 (TFOpLambda)     (1, 128, 128)        0           tf.signal.ifft2d_72[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_73 (TFOpLambda)     (1, 128, 128)        0           tf.signal.ifft2d_73[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_74 (TFOpLambda)     (1, 128, 128)        0           tf.signal.ifft2d_74[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_72 (TFOpLambda) (1, 128, 128)        0           tf.math.abs_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_73 (TFOpLambda) (1, 128, 128)        0           tf.math.abs_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_74 (TFOpLambda) (1, 128, 128)        0           tf.math.abs_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_242 (InputLayer)          [(1, 128, 128, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_243 (InputLayer)          [(1, 128, 128, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.stack_24 (TFOpLambda)        (1, 128, 128, 3)     0           tf.math.truediv_72[0][0]         \n",
      "                                                                 tf.math.truediv_73[0][0]         \n",
      "                                                                 tf.math.truediv_74[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "postprocessing_model = create_postprocessing_model()\n",
    "postprocessing_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Train loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_mean_aias_value_of_one_image(x_filmed, x_clean):\n",
    "    # print(\"betrete calc_mean_aias_value_of_one_image\")\n",
    "\n",
    "    # print(\"x_filmed: \")\n",
    "    # plt.imshow(x_filmed)\n",
    "    # plt.show()\n",
    "    # print(\"x_clean: \")\n",
    "    # plt.imshow(x_clean)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    fourier_handler = Fourier_Images(x_filmed, x_clean)\n",
    "\n",
    "    img_filmed_r, img_filmed_g, img_filmed_b = fourier_handler.split_RGB_2_Grayscale(\n",
    "        x_filmed)\n",
    "    img_clean_r, img_clean_g, img_clean_b = fourier_handler.split_RGB_2_Grayscale(\n",
    "        x_clean)\n",
    "\n",
    "    img_filmed_complex_r = fourier_handler.grayscale_2_Fourier(\n",
    "        img_filmed_r)\n",
    "    img_filmed_complex_g = fourier_handler.grayscale_2_Fourier(\n",
    "        img_filmed_g)\n",
    "    img_filmed_complex_b = fourier_handler.grayscale_2_Fourier(\n",
    "        img_filmed_b)\n",
    "\n",
    "    img_clean_complex_r = fourier_handler.grayscale_2_Fourier(\n",
    "        img_clean_r)\n",
    "    img_clean_complex_g = fourier_handler.grayscale_2_Fourier(\n",
    "        img_clean_g)\n",
    "    img_clean_complex_b = fourier_handler.grayscale_2_Fourier(\n",
    "        img_clean_b)\n",
    "\n",
    "\n",
    "    x_filmed_fourier_px, x_clean_fourier_px, differenzbild_fourier_px, img_filmed_fourier_combined, img_clean_fourier_combined = fourier_handler.generate_mask_from_images()\n",
    "    # plt.imsave(\".\\\\tmp\\\\img_filmed_fourier_combined.png\",\n",
    "    #            img_filmed_fourier_combined, cmap=\"gray\")\n",
    "    # plt.imsave(\".\\\\tmp\\\\img_clean_fourier_combined.png\",\n",
    "    #            img_clean_fourier_combined, cmap=\"gray\")\n",
    "    if show_intermediate_pics:\n",
    "        print(\"differenzbild_fourier_px: \")\n",
    "        # print(differenzbild_fourier_px)\n",
    "        plt.imshow(differenzbild_fourier_px, cmap=\"gray\")\n",
    "        plt.show()\n",
    "    differenzbild_fourier_px = differenzbild_fourier_px.reshape(\n",
    "        1, IMG_WIDTH, IMG_HEIGHT, 1)\n",
    "\n",
    "\n",
    "\n",
    "    del(fourier_handler)\n",
    "\n",
    "\n",
    "\n",
    "    # ----------- HIER STARTET U-NET-MODEL -----------\n",
    "\n",
    "    u_net_output = execute_UNet_model(\n",
    "        differenzbild_fourier_px, training=True)\n",
    "\n",
    "    # print(\"u_net_output: \")\n",
    "    # print(u_net_output)\n",
    "\n",
    "    # ----------- ALPHA-BLENDING -----------\n",
    "\n",
    "\n",
    "    # CASTING IST NICHT ABLEITBAR - KANN MAN NICHT VERWENDEN!!\n",
    "    # u_net_output = tf.cast(u_net_output, tf.int32)\n",
    "    # u_net_output = tf.cast(u_net_output, tf.bool)\n",
    "\n",
    "    # FORMEL: (1 - out) * clean + out * filmed\n",
    "    # t2c -> transfered to complex\n",
    "\n",
    "\n",
    "    if show_intermediate_pics:\n",
    "        print(\"x_clean: \")\n",
    "        plt.imshow(x_clean)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"x_filmed: \")\n",
    "        plt.imshow(x_filmed)\n",
    "        plt.show()\n",
    "\n",
    "    x_clean = x_clean.reshape((1, IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    x_filmed = x_filmed.reshape((1, IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "\n",
    "\n",
    "    u_net_output = tf.reshape(\n",
    "        u_net_output, shape=(IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "    img_clean_complex_r = img_clean_complex_r.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_clean_complex_g = img_clean_complex_g.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_clean_complex_b = img_clean_complex_b.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_filmed_complex_r = img_filmed_complex_r.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_filmed_complex_g = img_filmed_complex_g.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_filmed_complex_b = img_filmed_complex_b.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "\n",
    "    # ------------ postprocessing Model aufrufen --------\n",
    "\n",
    "    image_processed_rgb = execute_postprocessing_model([\n",
    "        x_clean ,\n",
    "        x_filmed ,\n",
    "        u_net_output,\n",
    "        img_clean_complex_r ,\n",
    "        img_clean_complex_g ,\n",
    "        img_clean_complex_b ,\n",
    "        img_filmed_complex_r ,\n",
    "        img_filmed_complex_g ,\n",
    "        img_filmed_complex_b ,\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if show_intermediate_pics:\n",
    "        print(\"image_processed_rgb: \")\n",
    "        # print(np.array(image_processed_rgb).reshape(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "        plt.imshow(np.array(image_processed_rgb).reshape(\n",
    "            IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # ----------- Feed multiple Buckets in CNN for predicting amount of alias -----------\n",
    "\n",
    "    def create_multiple_buckets_to_feed_them_into_CNN(tensor):\n",
    "\n",
    "        BATCH_SIZE_CNN = 1\n",
    "        NUM_BOXES = 30\n",
    "        CROP_SIZE = (60, 60)\n",
    "\n",
    "        boxes = tf.random.uniform(shape=(NUM_BOXES, 4))\n",
    "        box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0,\n",
    "                                        maxval=BATCH_SIZE_CNN, dtype=tf.int32)\n",
    "        \n",
    "        output = tf.image.crop_and_resize(\n",
    "            tensor, boxes, box_indices, CROP_SIZE)\n",
    "        return output\n",
    "\n",
    "    cnn_input = create_multiple_buckets_to_feed_them_into_CNN(image_processed_rgb)\n",
    "\n",
    "    # print(\"cnn_input: \")\n",
    "    # print(cnn_input)\n",
    "\n",
    "\n",
    "\n",
    "    # ------- Make prediction --------\n",
    "\n",
    "    y_pred = execute_cnn_model(cnn_input)\n",
    "\n",
    "    y_pred = tf.math.reduce_mean(\n",
    "        y_pred, keepdims=True\n",
    "    )\n",
    "\n",
    "    # print(\"Ende von calc_mean_aias_value_of_one_image:\")\n",
    "    # print(\"y_pred: \")\n",
    "    # print(y_pred)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|                                   | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next batch...\n",
      "WARNING:tensorflow:Model was constructed with shape (1, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(1, 128, 128), dtype=tf.float32, name='input_244'), name='input_244', description=\"created by layer 'input_244'\"), but it was called on an input with incompatible shape (128, 128).\n",
      "cnn_mean_prediction_values: \n",
      "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.20015262]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.26633412]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.23929459]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:  33%|█████████                  | 1/3 [00:03<00:07,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.23929458856582642\n",
      "next batch...\n",
      "cnn_mean_prediction_values: \n",
      "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.30645454]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.30022302]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.22169787]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:  67%|██████████████████         | 2/3 [00:06<00:03,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.23049622774124146\n",
      "next batch...\n",
      "cnn_mean_prediction_values: \n",
      "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.18986242]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.22129413]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.17581873]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████| 3/3 [00:09<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.21227039396762848\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|                                   | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next batch...\n",
      "cnn_mean_prediction_values: \n",
      "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.17081642]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.20731238]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.18324001]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:  33%|█████████                  | 1/3 [00:02<00:05,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.20501279830932617\n",
      "next batch...\n",
      "cnn_mean_prediction_values: \n",
      "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.23172556]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.19587389]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.18792254]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:  67%|██████████████████         | 2/3 [00:05<00:02,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.201594740152359\n",
      "next batch...\n",
      "cnn_mean_prediction_values: \n",
      "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.13808282]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.08286986]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.14873885]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████| 3/3 [00:08<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.19278542697429657\n",
      "Start of epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|                                   | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next batch...\n",
      "cnn_mean_prediction_values: \n",
      "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.05828899]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.07792763]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.04848271]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:  33%|█████████                  | 1/3 [00:02<00:05,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.1721707433462143\n",
      "next batch...\n",
      "cnn_mean_prediction_values: \n",
      "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.07157353]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.05025433]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.05182517]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:  67%|██████████████████         | 2/3 [00:05<00:02,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.15712754428386688\n",
      "next batch...\n",
      "cnn_mean_prediction_values: \n",
      "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.00980611]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.01822498]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.01999551]], dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████| 3/3 [00:08<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.14189065992832184\n",
      "overall_train_loss: 0.14189065992832184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom train loop\n",
    "\n",
    "show_intermediate_pics = False\n",
    "\n",
    "RGB_WEIGHTS = [0.299, 0.587, 0.114]\n",
    "\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "\n",
    "execute_UNet_model = tf.function(model_u_net)\n",
    "execute_cnn_model = tf.function(model_cnn)\n",
    "execute_postprocessing_model = tf.function(postprocessing_model)\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "batch_size_cnn = 1\n",
    "batch_size_unet = 3\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "epoch_loss = tf.keras.metrics.MeanAbsoluteError()\n",
    "# epoch_loss = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "# epoch_loss = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_writer = tf.summary.create_file_writer(\"u_net_logs/train/\")\n",
    "test_writer = tf.summary.create_file_writer(\"u_net_logs/test/\")\n",
    "\n",
    "y_true = tf.constant(0, dtype=tf.float16, name=\"y_true\")\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print(f\"Start of epoch {epoch}\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------- BATCHES SAMMELN Start ----------------------------------------------------------------------------------------\n",
    "    train_step = 0\n",
    "\n",
    "    for batch_idx in tqdm(range(batch_size_unet), desc=\"training...\", ascii=False, ncols=75):\n",
    "        print(\"next batch...\")\n",
    "        cnn_mean_prediction_values = []\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "        \n",
    "            # Bilder für den nächsten Batch sammeln:\n",
    "            for i in range(batch_size_unet):\n",
    "                x_filmed = train_filmed_img_gen_obj.__next__()\n",
    "                x_clean = train_clean_img_gen_obj.__next__()\n",
    "                \n",
    "                y_pred = calc_mean_aias_value_of_one_image(x_filmed, x_clean)\n",
    "                cnn_mean_prediction_values.append(y_pred)\n",
    "\n",
    "                # print(\"y_pred: \")\n",
    "                # print(y_pred)\n",
    "\n",
    "                # ---------------------------------------------------------------------------- BATCHES SAMMELN ENDE ----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "            # ----------- Calc loss -----------\n",
    "\n",
    "            y_true = tf.zeros(batch_size_unet, 1)\n",
    "\n",
    "            print(\"cnn_mean_prediction_values: \")\n",
    "            print(cnn_mean_prediction_values)\n",
    "\n",
    "            loss = loss_fn(y_true=y_true, y_pred=cnn_mean_prediction_values)\n",
    "            # print(\"loss: \")\n",
    "            # print(loss)\n",
    "\n",
    "            gradients = tape.gradient(loss, model_u_net.trainable_weights)\n",
    "            del cnn_mean_prediction_values[:]\n",
    "            # print(\"gradients: \")\n",
    "            # print(gradients)\n",
    "\n",
    "            # Optimize the model:\n",
    "            optimizer.apply_gradients(\n",
    "                zip(gradients, model_u_net.trainable_variables))\n",
    "\n",
    "            tmp = epoch_loss.update_state(y_true, y_pred)\n",
    "\n",
    "            # print(\"tmp\")\n",
    "            # print(tmp)\n",
    "\n",
    "\n",
    "            # print(\"------------------------------------\")\n",
    "            # print(\"y_true:\")\n",
    "            # print(y_true)\n",
    "\n",
    "            # print(\"y_pred\")\n",
    "            # print(y_pred)\n",
    "            # print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "            # with train_writer.as_default():\n",
    "            #     tf.summary.scalar(\"Loss\", loss, step=train_step)\n",
    "            #     tf.summary.scalar(\n",
    "            #         \"Accuracy\", epoch_loss.result(), step=train_step,\n",
    "            #     )\n",
    "            # train_step += 1\n",
    "        \n",
    "        # End Epoch\n",
    "        train_accuracy_results.append(epoch_loss.result())\n",
    "        print(f\"--------------- epoch_loss: {epoch_loss.result()}\")\n",
    "\n",
    "\n",
    "\n",
    "overall_train_loss = epoch_loss.result()\n",
    "print(f\"overall_train_loss: {overall_train_loss}\")\n",
    "epoch_loss.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 65536 into shape (1,128,128,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Main\\MA_PROGR\\Code_Fourier\\UNet_for_Fourier.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000020?line=0'>1</a>\u001b[0m test_img \u001b[39m=\u001b[39m img[\u001b[39m400\u001b[39m:\u001b[39m528\u001b[39m, \u001b[39m400\u001b[39m:\u001b[39m528\u001b[39m,:]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000020?line=1'>2</a>\u001b[0m test_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(test_img)\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m128\u001b[39;49m , \u001b[39m128\u001b[39;49m , \u001b[39m3\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000020?line=3'>4</a>\u001b[0m \u001b[39m# plt.imshow(test_img)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000020?line=5'>6</a>\u001b[0m test_img\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(test_img, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 65536 into shape (1,128,128,3)"
     ]
    }
   ],
   "source": [
    "test_img = img[400:528, 400:528,:]\n",
    "test_img = np.array(test_img).reshape(1, 128 , 128 , 3)\n",
    "\n",
    "# plt.imshow(test_img)\n",
    "\n",
    "test_img= tf.convert_to_tensor(test_img, dtype=tf.float32)\n",
    "\n",
    "test_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22300133f10>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "BATCH_SIZE = 1\n",
    "NUM_BOXES = 3\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "CHANNELS = 3\n",
    "CROP_SIZE = (24, 24)\n",
    "\n",
    "test_img = img[400:528, 400:528, :]\n",
    "test_img = np.array(test_img).reshape(1, 128, 128, 3)\n",
    "\n",
    "# plt.imshow(test_img)\n",
    "\n",
    "test_img = tf.convert_to_tensor(test_img, dtype=tf.float32)\n",
    "image = test_img\n",
    "boxes = tf.random.uniform(shape=(NUM_BOXES, 4))\n",
    "# plt.imshow(np.array(image).reshape(IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\n",
    "# plt.show()\n",
    "box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0,\n",
    "                                maxval=BATCH_SIZE, dtype=tf.int32)\n",
    "output = tf.image.crop_and_resize(image, boxes, box_indices, CROP_SIZE)\n",
    "output.shape  # => (5, 24, 24, 3)\n",
    "\n",
    "plt.imshow(output[2])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe6f1f0ffae94815f8b68c5970304948992b116503ec65631f55d524065bfaa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf_training')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
