{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import  InputLayer, Conv2D, Lambda, Dropout, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from UNet_Fourier_Facilities import Fourier_Images\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654105594\n"
     ]
    }
   ],
   "source": [
    "print(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = tf.keras.models.load_model(\"./cnn_models/single_rgb_image_regression_V02_epochs_100_1653595623\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cnn = tf.keras.models.load_model(\"./models/single_rgb_image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img1 = Image.open(\n",
    "#     \"D:\\Main\\MA_PROGR\\Data\\Train\\LED_Wand_Aufnahmen\\Alias\\LED_Wand_20001.png\")\n",
    "# img1 = np.asarray(img1)/255\n",
    "# img1 = img1.reshape(1, 60, 60, 3)\n",
    "# res1 = model.predict(img1)\n",
    "# print(res1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "IMG_CHANNELS = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILMED_PATH = \"D:\\\\Main\\\\MA_PROGR\\\\Data\\\\Train\\\\UNet_Train\\\\model_at_tree_100_pics\\\\filmed\"\n",
    "TRAIN_CLEAN_PATH = \"D:\\\\Main\\\\MA_PROGR\\\\Data\\\\Train\\\\UNet_Train\\\\model_at_tree_100_pics\\\\clean_aligned\"\n",
    "# TEST_PATH = \"./Data/data-science-bowl-2018/stage1_test/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filmed_imgs = []\n",
    "train_clean_imgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSET = 200\n",
    "abbruch_idx = 30\n",
    "\n",
    "\n",
    "def my_train_filmed_gen():\n",
    "    for i, addr_filmed in enumerate(os.listdir(TRAIN_FILMED_PATH)):\n",
    "        img =  plt.imread(f\"{TRAIN_FILMED_PATH}\\{addr_filmed}\")\n",
    "        yield img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3]\n",
    "\n",
    "def my_train_clean_gen():\n",
    "    for i, addr_clean in enumerate(os.listdir(TRAIN_CLEAN_PATH)):\n",
    "        img =  plt.imread(f\"{TRAIN_CLEAN_PATH}\\{addr_clean}\")\n",
    "        yield img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3]\n",
    "\n",
    "\n",
    "train_filmed_img_gen_obj = my_train_filmed_gen()\n",
    "train_clean_img_gen_obj = my_train_clean_gen()\n",
    "\n",
    "# for i, addr_filmed in enumerate( os.listdir(TRAIN_FILMED_PATH)):\n",
    "#     if i > 5:\n",
    "#         break\n",
    "#     img = plt.imread(f\"{TRAIN_FILMED_PATH}\\{addr_filmed}\")\n",
    "\n",
    "#     # :3 falls es Alpha Channel gibt (der soll weg)\n",
    "#     train_filmed_imgs.append(\n",
    "#         img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3])\n",
    "\n",
    "# for i , addr_clean in enumerate(os.listdir(TRAIN_CLEAN_PATH)):\n",
    "#     if i > 5:\n",
    "#         break\n",
    "#     # :3 falls es Alpha Channel gibt (der soll weg)\n",
    "#     img = plt.imread(f\"{TRAIN_CLEAN_PATH}\\{addr_clean}\")\n",
    "#     train_clean_imgs.append(\n",
    "#         img[OFFSET:IMG_WIDTH+OFFSET, OFFSET:IMG_HEIGHT+OFFSET, :3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_filmed_img_gen_obj.__next__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_clean_img_gen_obj.__next__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_clean_imgs[0])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build U-Net-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_38\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_381 (InputLayer)          [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 128, 128, 1)  0           input_381[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_722 (Conv2D)             (None, 128, 128, 16) 160         lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_342 (Dropout)           (None, 128, 128, 16) 0           conv2d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_723 (Conv2D)             (None, 128, 128, 16) 2320        dropout_342[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_152 (MaxPooling2D (None, 64, 64, 16)   0           conv2d_723[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_724 (Conv2D)             (None, 64, 64, 32)   4640        max_pooling2d_152[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_343 (Dropout)           (None, 64, 64, 32)   0           conv2d_724[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_725 (Conv2D)             (None, 64, 64, 32)   9248        dropout_343[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_153 (MaxPooling2D (None, 32, 32, 32)   0           conv2d_725[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_726 (Conv2D)             (None, 32, 32, 64)   18496       max_pooling2d_153[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_344 (Dropout)           (None, 32, 32, 64)   0           conv2d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_727 (Conv2D)             (None, 32, 32, 64)   36928       dropout_344[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_154 (MaxPooling2D (None, 16, 16, 64)   0           conv2d_727[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_728 (Conv2D)             (None, 16, 16, 128)  73856       max_pooling2d_154[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_345 (Dropout)           (None, 16, 16, 128)  0           conv2d_728[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_729 (Conv2D)             (None, 16, 16, 128)  147584      dropout_345[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_155 (MaxPooling2D (None, 8, 8, 128)    0           conv2d_729[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_730 (Conv2D)             (None, 8, 8, 256)    295168      max_pooling2d_155[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_346 (Dropout)           (None, 8, 8, 256)    0           conv2d_730[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_731 (Conv2D)             (None, 8, 8, 256)    590080      dropout_346[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_152 (Conv2DTra (None, 16, 16, 128)  131200      conv2d_731[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_152 (Concatenate)   (None, 16, 16, 256)  0           conv2d_transpose_152[0][0]       \n",
      "                                                                 conv2d_729[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_732 (Conv2D)             (None, 16, 16, 128)  295040      concatenate_152[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_347 (Dropout)           (None, 16, 16, 128)  0           conv2d_732[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_733 (Conv2D)             (None, 16, 16, 128)  147584      dropout_347[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_153 (Conv2DTra (None, 32, 32, 64)   32832       conv2d_733[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_153 (Concatenate)   (None, 32, 32, 128)  0           conv2d_transpose_153[0][0]       \n",
      "                                                                 conv2d_727[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_734 (Conv2D)             (None, 32, 32, 64)   73792       concatenate_153[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_348 (Dropout)           (None, 32, 32, 64)   0           conv2d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_735 (Conv2D)             (None, 32, 32, 64)   36928       dropout_348[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_154 (Conv2DTra (None, 64, 64, 32)   8224        conv2d_735[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_154 (Concatenate)   (None, 64, 64, 64)   0           conv2d_transpose_154[0][0]       \n",
      "                                                                 conv2d_725[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_736 (Conv2D)             (None, 64, 64, 32)   18464       concatenate_154[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_349 (Dropout)           (None, 64, 64, 32)   0           conv2d_736[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_737 (Conv2D)             (None, 64, 64, 32)   9248        dropout_349[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_155 (Conv2DTra (None, 128, 128, 16) 2064        conv2d_737[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_155 (Concatenate)   (None, 128, 128, 32) 0           conv2d_transpose_155[0][0]       \n",
      "                                                                 conv2d_723[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_738 (Conv2D)             (None, 128, 128, 16) 4624        concatenate_155[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_350 (Dropout)           (None, 128, 128, 16) 0           conv2d_738[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_739 (Conv2D)             (None, 128, 128, 16) 2320        dropout_350[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_740 (Conv2D)             (None, 128, 128, 1)  17          conv2d_739[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,940,817\n",
      "Trainable params: 1,940,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(IMG_WIDTH, IMG_HEIGHT, 1), batch_size=None)\n",
    "\n",
    "# Hier werden Preprocessing-Schritte ausgeführt\n",
    "# s ist hier dann Differnzbild (Pixelraum_Fourier)\n",
    "s = Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "\n",
    "# Contraction path\n",
    "c1 = Conv2D(\n",
    "    16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "c1 = Dropout(0.1)(c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c1)\n",
    "p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p1)\n",
    "c2 = Dropout(0.1)(c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c2)\n",
    "p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p2)\n",
    "c3 = Dropout(0.2)(c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c3)\n",
    "p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p3)\n",
    "c4 = Dropout(0.2)(c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "c5 = Conv2D(256, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(p4)\n",
    "c5 = Dropout(0.3)(c5)\n",
    "c5 = Conv2D(256, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "# Expansive path\n",
    "u6 = Conv2DTranspose(\n",
    "    128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(u6)\n",
    "c6 = Dropout(0.2)(c6)\n",
    "c6 = Conv2D(128, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "u7 = Conv2DTranspose(\n",
    "    64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(64, (3, 3), activation='relu',\n",
    "            kernel_initializer='he_normal', padding='same')(u7)\n",
    "c7 = Dropout(0.2)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "u8 = tf.keras.layers.Conv2DTranspose(\n",
    "    32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(u8)\n",
    "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "u9 = tf.keras.layers.Conv2DTranspose(\n",
    "    16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(u9)\n",
    "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu',\n",
    "                            kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "model_u_net = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "model_u_net.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppm: postprocessing model \n",
    "\n",
    "def create_postprocessing_model():\n",
    "\n",
    "    ppm_input_img_clean = tf.keras.Input(shape=(128,128, 3), batch_size=1)\n",
    "    ppm_input_img_filmed = tf.keras.Input(shape=(128,128, 3), batch_size=1)\n",
    "    ppm_input_unet_output = tf.keras.Input(shape=(128,128), batch_size=1)\n",
    "\n",
    "    ppm_input_img_clean_complex_r = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_clean_complex_g = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_clean_complex_b = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "\n",
    "    ppm_input_img_filmed_complex_r = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_filmed_complex_g = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "    ppm_input_img_filmed_complex_b = tf.keras.Input(shape=(128,128), batch_size=1, dtype=tf.complex64)\n",
    "\n",
    "\n",
    "\n",
    "    ones = tf.ones((128, 128))\n",
    "    zeros = tf.zeros((128, 128))\n",
    "    ones_t2c = tf.complex(ones, zeros)\n",
    "    u_net_output_t2c = tf.complex(ppm_input_unet_output, zeros)\n",
    "\n",
    "    # FORMEL: (1 - out) * clean + out * filmed\n",
    "    # out == mask \n",
    "    # t2c -> transfered to complex\n",
    "\n",
    "\n",
    "    def soft_blending(clean, filmed, ones_t2c=ones_t2c, u_net_output_t2c=u_net_output_t2c):\n",
    "        zw1 = tf.math.subtract(ones_t2c, u_net_output_t2c)\n",
    "        zw1 = tf.math.multiply(zw1, clean)\n",
    "\n",
    "        zw2 = tf.multiply(u_net_output_t2c, filmed)\n",
    "        return tf.math.add(zw1, zw2)\n",
    "\n",
    "\n",
    "    img_processed_complex_fourier_r = soft_blending(\n",
    "        ppm_input_img_clean_complex_r, ppm_input_img_filmed_complex_r)\n",
    "\n",
    "    img_processed_complex_fourier_g = soft_blending(\n",
    "        ppm_input_img_clean_complex_g, ppm_input_img_filmed_complex_g)\n",
    "\n",
    "    img_processed_complex_fourier_b = soft_blending(\n",
    "        ppm_input_img_clean_complex_b, ppm_input_img_filmed_complex_b)\n",
    "\n",
    "\n",
    "\n",
    "    # # img_processed_px_fourier = tf.math.log(                     # zum anschauen\n",
    "    # #     tf.math.abs(img_processed_complex_fourier_r))\n",
    "\n",
    "    # ----------- INVERSE FOURIER TRANSFORMATION -----------\n",
    "\n",
    "    img_processed_r = tf.signal.ifft2d(\n",
    "        img_processed_complex_fourier_r\n",
    "    )\n",
    "    img_processed_g = tf.signal.ifft2d(\n",
    "        img_processed_complex_fourier_g\n",
    "    )\n",
    "    img_processed_b = tf.signal.ifft2d(\n",
    "        img_processed_complex_fourier_b\n",
    "    )\n",
    "\n",
    "\n",
    "    # 3 Einzelkanäle zu einem RGB Bild\n",
    "    img_processed_r = tf.math.abs(img_processed_r)/255\n",
    "    img_processed_g = tf.math.abs(img_processed_g)/255\n",
    "    img_processed_b = tf.math.abs(img_processed_b)/255\n",
    "\n",
    "    img_processed_rgb = tf.stack(\n",
    "        [img_processed_r, img_processed_g, img_processed_b], axis=3)\n",
    "\n",
    "    postprocess_model = tf.keras.Model(inputs=[\n",
    "        ppm_input_img_clean, \n",
    "        ppm_input_img_filmed,\n",
    "        ppm_input_unet_output,\n",
    "        ppm_input_img_clean_complex_r ,\n",
    "        ppm_input_img_clean_complex_g ,\n",
    "        ppm_input_img_clean_complex_b ,\n",
    "        ppm_input_img_filmed_complex_r, \n",
    "        ppm_input_img_filmed_complex_g,\n",
    "        ppm_input_img_filmed_complex_b\n",
    "    ], outputs=[img_processed_rgb], name=\"postprocessing_model\")\n",
    "\n",
    "    # postprocess_model.summary()\n",
    "    return postprocess_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_postprocessing_model():\n",
    "#     clean = tf.keras.Input(shape=(128, 128, 3), batch_size=1)\n",
    "#     darker = tf.math.multiply(clean, 3)\n",
    "\n",
    "#     postprocessing_model = tf.keras.Model(inputs=[clean], outputs=[darker])\n",
    "\n",
    "#     return postprocessing_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"postprocessing_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_384 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.dtypes.complex_38 (TFOpLambd (1, 128, 128)        0           input_384[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_114 (TFOpLambd (1, 128, 128)        0           tf.dtypes.complex_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_385 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_388 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_115 (TFOpLambd (1, 128, 128)        0           tf.dtypes.complex_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_386 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_389 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_116 (TFOpLambd (1, 128, 128)        0           tf.dtypes.complex_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_387 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_390 (InputLayer)          [(1, 128, 128)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_228 (TFOpLambd (1, 128, 128)        0           tf.math.subtract_114[0][0]       \n",
      "                                                                 input_385[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_229 (TFOpLambd (1, 128, 128)        0           tf.dtypes.complex_38[0][0]       \n",
      "                                                                 input_388[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_230 (TFOpLambd (1, 128, 128)        0           tf.math.subtract_115[0][0]       \n",
      "                                                                 input_386[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_231 (TFOpLambd (1, 128, 128)        0           tf.dtypes.complex_38[0][0]       \n",
      "                                                                 input_389[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_232 (TFOpLambd (1, 128, 128)        0           tf.math.subtract_116[0][0]       \n",
      "                                                                 input_387[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_233 (TFOpLambd (1, 128, 128)        0           tf.dtypes.complex_38[0][0]       \n",
      "                                                                 input_390[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_114 (TFOpLambda)    (1, 128, 128)        0           tf.math.multiply_228[0][0]       \n",
      "                                                                 tf.math.multiply_229[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_115 (TFOpLambda)    (1, 128, 128)        0           tf.math.multiply_230[0][0]       \n",
      "                                                                 tf.math.multiply_231[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.add_116 (TFOpLambda)    (1, 128, 128)        0           tf.math.multiply_232[0][0]       \n",
      "                                                                 tf.math.multiply_233[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.signal.ifft2d_114 (TFOpLambd (1, 128, 128)        0           tf.math.add_114[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.signal.ifft2d_115 (TFOpLambd (1, 128, 128)        0           tf.math.add_115[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.signal.ifft2d_116 (TFOpLambd (1, 128, 128)        0           tf.math.add_116[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_114 (TFOpLambda)    (1, 128, 128)        0           tf.signal.ifft2d_114[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_115 (TFOpLambda)    (1, 128, 128)        0           tf.signal.ifft2d_115[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_116 (TFOpLambda)    (1, 128, 128)        0           tf.signal.ifft2d_116[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_114 (TFOpLambda (1, 128, 128)        0           tf.math.abs_114[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_115 (TFOpLambda (1, 128, 128)        0           tf.math.abs_115[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_116 (TFOpLambda (1, 128, 128)        0           tf.math.abs_116[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_382 (InputLayer)          [(1, 128, 128, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_383 (InputLayer)          [(1, 128, 128, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.stack_38 (TFOpLambda)        (1, 128, 128, 3)     0           tf.math.truediv_114[0][0]        \n",
      "                                                                 tf.math.truediv_115[0][0]        \n",
      "                                                                 tf.math.truediv_116[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "postprocessing_model = create_postprocessing_model()\n",
    "postprocessing_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Train loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_mean_alias_value_of_one_image(x_filmed, x_clean):\n",
    "    # print(\"betrete calc_mean_aias_value_of_one_image\")\n",
    "\n",
    "    # print(\"x_filmed: \")\n",
    "    # plt.imshow(x_filmed)\n",
    "    # plt.show()\n",
    "    # print(\"x_clean: \")\n",
    "    # plt.imshow(x_clean)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    fourier_handler = Fourier_Images(x_filmed, x_clean)\n",
    "\n",
    "    img_filmed_r, img_filmed_g, img_filmed_b = fourier_handler.split_RGB_2_Grayscale(\n",
    "        x_filmed)\n",
    "    img_clean_r, img_clean_g, img_clean_b = fourier_handler.split_RGB_2_Grayscale(\n",
    "        x_clean)\n",
    "\n",
    "    img_filmed_complex_r = fourier_handler.grayscale_2_Fourier(\n",
    "        img_filmed_r)\n",
    "    img_filmed_complex_g = fourier_handler.grayscale_2_Fourier(\n",
    "        img_filmed_g)\n",
    "    img_filmed_complex_b = fourier_handler.grayscale_2_Fourier(\n",
    "        img_filmed_b)\n",
    "\n",
    "    img_clean_complex_r = fourier_handler.grayscale_2_Fourier(\n",
    "        img_clean_r)\n",
    "    img_clean_complex_g = fourier_handler.grayscale_2_Fourier(\n",
    "        img_clean_g)\n",
    "    img_clean_complex_b = fourier_handler.grayscale_2_Fourier(\n",
    "        img_clean_b)\n",
    "\n",
    "\n",
    "    x_filmed_fourier_px, x_clean_fourier_px, differenzbild_fourier_px, img_filmed_fourier_combined, img_clean_fourier_combined = fourier_handler.generate_mask_from_images()\n",
    "    # plt.imsave(\".\\\\tmp\\\\img_filmed_fourier_combined.png\",\n",
    "    #            img_filmed_fourier_combined, cmap=\"gray\")\n",
    "    # plt.imsave(\".\\\\tmp\\\\img_clean_fourier_combined.png\",\n",
    "    #            img_clean_fourier_combined, cmap=\"gray\")\n",
    "    if show_intermediate_pics:\n",
    "        print(\"differenzbild_fourier_px: \")\n",
    "        # print(differenzbild_fourier_px)\n",
    "        plt.imshow(differenzbild_fourier_px, cmap=\"gray\")\n",
    "        plt.show()\n",
    "\n",
    "    differenzbild_fourier_px = differenzbild_fourier_px.reshape(\n",
    "        1, IMG_WIDTH, IMG_HEIGHT, 1)    \n",
    "\n",
    "\n",
    "\n",
    "    del(fourier_handler)\n",
    "\n",
    "\n",
    "\n",
    "    # ----------- HIER STARTET U-NET-MODEL -----------\n",
    "\n",
    "    u_net_output = execute_UNet_model(\n",
    "        differenzbild_fourier_px, training=True)\n",
    "\n",
    "\n",
    "    # print(\"u_net_output: \")\n",
    "    # print(u_net_output)\n",
    "\n",
    "    # ----------- ALPHA-BLENDING -----------\n",
    "\n",
    "\n",
    "    # CASTING IST NICHT ABLEITBAR - KANN MAN NICHT VERWENDEN!!\n",
    "    # u_net_output = tf.cast(u_net_output, tf.int32)\n",
    "    # u_net_output = tf.cast(u_net_output, tf.bool)\n",
    "\n",
    "    # FORMEL: (1 - out) * clean + out * filmed\n",
    "    # t2c -> transfered to complex\n",
    "\n",
    "\n",
    "    if show_intermediate_pics:\n",
    "        print(\"x_clean: \")\n",
    "        plt.imshow(x_clean)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"x_filmed: \")\n",
    "        plt.imshow(x_filmed)\n",
    "        plt.show()\n",
    "\n",
    "    x_clean = x_clean.reshape((1, IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    x_filmed = x_filmed.reshape((1, IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "\n",
    "\n",
    "    u_net_output = tf.reshape(\n",
    "        u_net_output, shape=(IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "    img_clean_complex_r = img_clean_complex_r.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_clean_complex_g = img_clean_complex_g.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_clean_complex_b = img_clean_complex_b.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_filmed_complex_r = img_filmed_complex_r.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_filmed_complex_g = img_filmed_complex_g.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    img_filmed_complex_b = img_filmed_complex_b.reshape((1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "\n",
    "    # ------------ postprocessing Model aufrufen --------\n",
    "\n",
    "    image_processed_rgb = execute_postprocessing_model([\n",
    "        x_clean ,\n",
    "        x_filmed ,\n",
    "        u_net_output,\n",
    "        img_clean_complex_r ,\n",
    "        img_clean_complex_g ,\n",
    "        img_clean_complex_b ,\n",
    "        img_filmed_complex_r ,\n",
    "        img_filmed_complex_g ,\n",
    "        img_filmed_complex_b ,\n",
    "    ])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if show_intermediate_pics:\n",
    "        print(\"image_processed_rgb: \")\n",
    "        # print(np.array(image_processed_rgb).reshape(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "        plt.imshow(np.array(image_processed_rgb).reshape(\n",
    "            IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # ----------- Feed multiple Buckets in CNN for predicting amount of alias -----------\n",
    "\n",
    "    def create_multiple_buckets_to_feed_them_into_CNN(tensor):\n",
    "\n",
    "        BATCH_SIZE_CNN = 1\n",
    "        NUM_BOXES = 30\n",
    "        CROP_SIZE = (60, 60)\n",
    "\n",
    "        boxes = tf.random.uniform(shape=(NUM_BOXES, 4))\n",
    "        box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0,\n",
    "                                        maxval=BATCH_SIZE_CNN, dtype=tf.int32)\n",
    "        \n",
    "        output = tf.image.crop_and_resize(\n",
    "            tensor, boxes, box_indices, CROP_SIZE)\n",
    "        return output\n",
    "\n",
    "    cnn_input = create_multiple_buckets_to_feed_them_into_CNN(image_processed_rgb)\n",
    "\n",
    "    # print(\"cnn_input: \")\n",
    "    # print(cnn_input)\n",
    "\n",
    "\n",
    "\n",
    "    # ------- Make prediction --------\n",
    "\n",
    "    y_pred = execute_cnn_model(cnn_input)\n",
    "\n",
    "    y_pred = tf.math.reduce_mean(\n",
    "        y_pred, keepdims=True\n",
    "    )\n",
    "\n",
    "    # print(\"Ende von calc_mean_aias_value_of_one_image:\")\n",
    "    # print(\"y_pred: \")\n",
    "    # print(y_pred)\n",
    "\n",
    "    return y_pred, differenzbild_fourier_px, u_net_output, image_processed_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|                                  | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next batch...\n",
      "WARNING:tensorflow:Model was constructed with shape (1, 128, 128) for input KerasTensor(type_spec=TensorSpec(shape=(1, 128, 128), dtype=tf.float32, name='input_384'), name='input_384', description=\"created by layer 'input_384'\"), but it was called on an input with incompatible shape (128, 128).\n",
      "cnn_mean_prediction_values.shape: \n",
      "(32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   3%|▊                         | 1/32 [00:41<21:11, 41.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.24206052720546722\n",
      "next batch...\n",
      "cnn_mean_prediction_values.shape: \n",
      "(32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   6%|█▋                        | 2/32 [01:23<20:59, 41.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.22056496143341064\n",
      "next batch...\n",
      "cnn_mean_prediction_values.shape: \n",
      "(32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   9%|██▍                       | 3/32 [02:06<20:30, 42.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.21840208768844604\n",
      "next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   9%|██▍                       | 3/32 [02:08<20:45, 42.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Am Ende angelangt, gehe in neue Epoche\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|                                  | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next batch...\n",
      "cnn_mean_prediction_values.shape: \n",
      "(32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   3%|▊                         | 1/32 [00:39<20:20, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.2070516049861908\n",
      "next batch...\n",
      "cnn_mean_prediction_values.shape: \n",
      "(32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   6%|█▋                        | 2/32 [01:18<19:36, 39.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.21082821488380432\n",
      "next batch...\n",
      "cnn_mean_prediction_values.shape: \n",
      "(32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   9%|██▍                       | 3/32 [01:57<18:54, 39.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.19433708488941193\n",
      "next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   9%|██▍                       | 3/32 [01:59<19:11, 39.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Am Ende angelangt, gehe in neue Epoche\n",
      "Start of epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|                                  | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next batch...\n",
      "cnn_mean_prediction_values.shape: \n",
      "(32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   3%|▊                         | 1/32 [00:39<20:23, 39.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.17724765837192535\n",
      "next batch...\n",
      "cnn_mean_prediction_values.shape: \n",
      "(32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   6%|█▋                        | 2/32 [01:19<19:54, 39.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.15604063868522644\n",
      "next batch...\n",
      "cnn_mean_prediction_values.shape: \n",
      "(32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   9%|██▍                       | 3/32 [01:58<18:59, 39.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- epoch_loss: 0.13992160558700562\n",
      "next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   9%|██▍                       | 3/32 [01:59<19:18, 39.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Am Ende angelangt, gehe in neue Epoche\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom train loop\n",
    "\n",
    "SAFE_RESULTS = True\n",
    "WRITE_PATH = \"D:\\\\Main\\\\MA_PROGR\\\\Data\\\\UNET_Output\\\\Session04\"\n",
    "\n",
    "show_intermediate_pics = False\n",
    "\n",
    "RGB_WEIGHTS = [0.299, 0.587, 0.114]\n",
    "\n",
    "\n",
    "overall_train_loss = []\n",
    "\n",
    "\n",
    "execute_UNet_model = tf.function(model_u_net)\n",
    "execute_cnn_model = tf.function(model_cnn)\n",
    "execute_postprocessing_model = tf.function(postprocessing_model)\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "batch_size_cnn = 1\n",
    "batch_size_unet = 32\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "epoch_loss = tf.keras.metrics.MeanAbsoluteError()\n",
    "# epoch_loss = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "# epoch_loss = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_writer = tf.summary.create_file_writer(\"u_net_logs/train/\")\n",
    "test_writer = tf.summary.create_file_writer(\"u_net_logs/test/\")\n",
    "\n",
    "y_true = tf.constant(0, dtype=tf.float16, name=\"y_true\")\n",
    "\n",
    "jump_to_new_epoch = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print(f\"Start of epoch {epoch}\")\n",
    "    jump_to_new_epoch = False\n",
    "\n",
    "    # Mache neue Generater, sodass wieder durch alle Bilder durchgegange wird für die nächste Epoche:\n",
    "    train_filmed_img_gen_obj = my_train_filmed_gen()\n",
    "    train_clean_img_gen_obj = my_train_clean_gen()\n",
    "\n",
    "    # ---------------------------------------------------------------------------- BATCHES SAMMELN Start ----------------------------------------------------------------------------------------\n",
    "    train_step = 0\n",
    "\n",
    "    for batch_idx in tqdm(range(batch_size_unet), desc=\"training...\", ascii=False, ncols=75):\n",
    "        print(\"next batch...\")\n",
    "        cnn_mean_prediction_values = []\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "        \n",
    "            # Bilder für den nächsten Batch sammeln:\n",
    "            for i in range(batch_size_unet):\n",
    "                try:\n",
    "                    x_filmed = train_filmed_img_gen_obj.__next__()\n",
    "                    x_clean = train_clean_img_gen_obj.__next__()\n",
    "                except StopIteration as e:\n",
    "                    print(\"------------------------Am Ende angelangt, gehe in neue Epoche\")\n",
    "\n",
    "                    jump_to_new_epoch = True\n",
    "\n",
    "                if jump_to_new_epoch == True:\n",
    "                    # Alle Bilder sind aufgebraucht, gehe in neue Epoche\n",
    "                    break\n",
    "\n",
    "\n",
    "                \n",
    "                y_pred, differenzbild_fourier_px, u_net_output, image_processed_rgb = calc_mean_alias_value_of_one_image(\n",
    "                    x_filmed, x_clean)\n",
    "                cnn_mean_prediction_values.append(y_pred)\n",
    "\n",
    "                # print(\"y_pred: \")\n",
    "                # print(y_pred)\n",
    "\n",
    "                # ---------------------------------------------------------------------------- BATCHES SAMMELN ENDE ----------------------------------------------------------------------------------------\n",
    "\n",
    "            \n",
    "            if SAFE_RESULTS:\n",
    "                plt.imsave(f\"{WRITE_PATH}\\\\x_filmed_{epoch}_{batch_idx}.png\", x_filmed)\n",
    "\n",
    "                plt.imsave(f\"{WRITE_PATH}\\\\differenzbild_{epoch}_{batch_idx}.png\",\n",
    "                   differenzbild_fourier_px.reshape((IMG_WIDTH,IMG_HEIGHT)), cmap=\"gray\")\n",
    "\n",
    "                plt.imsave(f\"{WRITE_PATH}\\\\unet_output_{epoch}_{batch_idx}.png\",\n",
    "                   u_net_output, cmap=\"gray\")\n",
    "\n",
    "\n",
    "                # Umformen in Numpy-Array\n",
    "                image_processed_rgb = np.array(image_processed_rgb).reshape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "                # Max-Wert liegt manchesmal über 1 (keine ahnung warum...)\n",
    "                if image_processed_rgb.max() >= 1:\n",
    "                    # Werte auf 0 - 1 bringen\n",
    "                    image_processed_rgb= image_processed_rgb / image_processed_rgb.max()\n",
    "\n",
    "\n",
    "                plt.imsave(f\"{WRITE_PATH}\\\\image_processed_rgb_{epoch}_{batch_idx}.png\",\n",
    "                   np.array(image_processed_rgb).reshape((IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "\n",
    "            if jump_to_new_epoch == True:\n",
    "                # Alle Bilder sind aufgebraucht, gehe in neue \n",
    "                break\n",
    "\n",
    "            # ----------- Calc loss -----------\n",
    "\n",
    "            y_true = tf.zeros(batch_size_unet, 1)\n",
    "\n",
    "            print(\"cnn_mean_prediction_values.shape: \")\n",
    "            print( np.array(cnn_mean_prediction_values).shape)\n",
    "\n",
    "            loss = loss_fn(y_true=y_true, y_pred=cnn_mean_prediction_values)\n",
    "            # print(\"loss: \")\n",
    "            # print(loss)\n",
    "\n",
    "            gradients = tape.gradient(loss, model_u_net.trainable_weights)\n",
    "            del cnn_mean_prediction_values[:]\n",
    "            # print(\"gradients: \")\n",
    "            # print(gradients)\n",
    "\n",
    "            # Optimize the model:\n",
    "            optimizer.apply_gradients(\n",
    "                zip(gradients, model_u_net.trainable_variables))\n",
    "\n",
    "            tmp = epoch_loss.update_state(y_true, y_pred)\n",
    "\n",
    "            # print(\"tmp\")\n",
    "            # print(tmp)\n",
    "\n",
    "\n",
    "            # print(\"------------------------------------\")\n",
    "            # print(\"y_true:\")\n",
    "            # print(y_true)\n",
    "\n",
    "            # print(\"y_pred\")\n",
    "            # print(y_pred)\n",
    "            # print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "            # with train_writer.as_default():\n",
    "            #     tf.summary.scalar(\"Loss\", loss, step=train_step)\n",
    "            #     tf.summary.scalar(\n",
    "            #         \"Accuracy\", epoch_loss.result(), step=train_step,\n",
    "            #     )\n",
    "            # train_step += 1\n",
    "        \n",
    "        # End Epoch\n",
    "\n",
    "        print(f\"--------------- epoch_loss: {epoch_loss.result()}\")\n",
    "        overall_train_loss.append(epoch_loss.result())\n",
    "\n",
    "\n",
    "\n",
    "# overall_train_loss = epoch_loss.result()\n",
    "# print(f\"overall_train_loss: {overall_train_loss}\")\n",
    "# epoch_loss.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22302998040>]"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plotsize()\n",
    "plt.plot(overall_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(overall_train_loss)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 65536 into shape (1,128,128,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Main\\MA_PROGR\\Code_Fourier\\UNet_for_Fourier.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000022?line=0'>1</a>\u001b[0m test_img \u001b[39m=\u001b[39m img[\u001b[39m400\u001b[39m:\u001b[39m528\u001b[39m, \u001b[39m400\u001b[39m:\u001b[39m528\u001b[39m,:]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000022?line=1'>2</a>\u001b[0m test_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(test_img)\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m128\u001b[39;49m , \u001b[39m128\u001b[39;49m , \u001b[39m3\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000022?line=3'>4</a>\u001b[0m \u001b[39m# plt.imshow(test_img)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Main/MA_PROGR/Code_Fourier/UNet_for_Fourier.ipynb#ch0000022?line=5'>6</a>\u001b[0m test_img\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(test_img, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 65536 into shape (1,128,128,3)"
     ]
    }
   ],
   "source": [
    "test_img = img[400:528, 400:528,:]\n",
    "test_img = np.array(test_img).reshape(1, 128 , 128 , 3)\n",
    "\n",
    "# plt.imshow(test_img)\n",
    "\n",
    "test_img= tf.convert_to_tensor(test_img, dtype=tf.float32)\n",
    "\n",
    "test_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22300133f10>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "BATCH_SIZE = 1\n",
    "NUM_BOXES = 3\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "CHANNELS = 3\n",
    "CROP_SIZE = (24, 24)\n",
    "\n",
    "test_img = img[400:528, 400:528, :]\n",
    "test_img = np.array(test_img).reshape(1, 128, 128, 3)\n",
    "\n",
    "# plt.imshow(test_img)\n",
    "\n",
    "test_img = tf.convert_to_tensor(test_img, dtype=tf.float32)\n",
    "image = test_img\n",
    "boxes = tf.random.uniform(shape=(NUM_BOXES, 4))\n",
    "# plt.imshow(np.array(image).reshape(IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\n",
    "# plt.show()\n",
    "box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0,\n",
    "                                maxval=BATCH_SIZE, dtype=tf.int32)\n",
    "output = tf.image.crop_and_resize(image, boxes, box_indices, CROP_SIZE)\n",
    "output.shape  # => (5, 24, 24, 3)\n",
    "\n",
    "plt.imshow(output[2])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe6f1f0ffae94815f8b68c5970304948992b116503ec65631f55d524065bfaa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf_training')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
